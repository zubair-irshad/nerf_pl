{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "dataset datasets.llff.LLFFDatasetNOCS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from models.rendering import *\n",
    "from models.nerf import *\n",
    "\n",
    "import metrics\n",
    "\n",
    "from datasets import dataset_dict\n",
    "from datasets.llff import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "img_wh = (640, 480)\n",
    "\n",
    "# dataset = dataset_dict['llff'] \\\n",
    "#           ('/home/ubuntu/data/nerf_example_data/nerf_llff_data/fern/', 'test_train', spheric_poses=False,\n",
    "#            img_wh=img_wh)\n",
    "\n",
    "# dataset = dataset_dict['llff_nocs'] \\\n",
    "#           ('data/scene_1',\n",
    "#            img_wh=img_wh)\n",
    "dataset = dataset_dict['llff_nocs']\n",
    "\n",
    "print(\"dataset\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.focal 584.5808479748345\n",
      "val image is data/scene_1/images/0023_color.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zubair/anaconda3/envs/nerf_pl/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "kwargs = {'root_dir': 'data/scene_1',\n",
    "          'img_wh': tuple(img_wh)}\n",
    "kwargs['spheric_poses'] = False\n",
    "kwargs['val_num'] = 1\n",
    "\n",
    "train_dataset = dataset(split='val', **kwargs)\n",
    "\n",
    "dataloader =  DataLoader(train_dataset,\n",
    "                  shuffle=True,\n",
    "                  num_workers=4,\n",
    "                  batch_size=1024,\n",
    "                  pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
    "[0.0, 0.4, 0.0, 0.0],\n",
    "[0.0, 0.0, 1.2, 0.0],\n",
    "[0.0, 0.0, 0.0,-0.4]])\n",
    "b = torch.nonzero(a)\n",
    "\n",
    "print(b.shape)\n",
    "c = np.nonzero(a.numpy())\n",
    "c = np.transpose(np.asarray(c))\n",
    "print(np.transpose(np.asarray(c)).shape)\n",
    "print(np.nonzero(a.numpy()))\n",
    "\n",
    "print(np.equal(b.numpy(), c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train dataset 1\n",
      "k, v rays torch.Size([1, 307200, 8])\n",
      "k, v rgbs torch.Size([1, 307200, 3])\n",
      "k, v instance_mask torch.Size([1, 307200])\n",
      "k, v instance_mask_weight torch.Size([1, 307200])\n",
      "k, v instance_ids torch.Size([1, 307200])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"len train dataset\", len(train_dataset))\n",
    "count = 0\n",
    "for data in dataloader:\n",
    "    for k,v in data.items():\n",
    "        print(\"k, v\", k, v.shape)\n",
    "    count+=1\n",
    "print(count)\n",
    "#     print(\"data_rgb\", data['rgbs'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_xyz = Embedding(10)\n",
    "embedding_dir = Embedding(4)\n",
    "\n",
    "nerf_coarse = NeRF()\n",
    "nerf_fine = NeRF()\n",
    "\n",
    "# ckpt_path = 'ckpts_old/fern/epoch=29.ckpt'\n",
    "ckpt_path = 'ckpts_old/lego/epoch=15.ckpt'\n",
    "\n",
    "load_ckpt(nerf_coarse, ckpt_path, model_name='nerf_coarse')\n",
    "load_ckpt(nerf_fine, ckpt_path, model_name='nerf_fine')\n",
    "\n",
    "nerf_coarse.cuda().eval()\n",
    "nerf_fine.cuda().eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'coarse': nerf_coarse, 'fine': nerf_fine}\n",
    "embeddings = {'xyz': embedding_xyz, 'dir': embedding_dir}\n",
    "\n",
    "N_samples = 64\n",
    "N_importance = 64\n",
    "use_disp = False\n",
    "chunk = 1024*32*4\n",
    "\n",
    "@torch.no_grad()\n",
    "def f(rays):\n",
    "    \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
    "    B = rays.shape[0]\n",
    "    results = defaultdict(list)\n",
    "    for i in range(0, B, chunk):\n",
    "        rendered_ray_chunks = \\\n",
    "            render_rays(models,\n",
    "                        embeddings,\n",
    "                        rays[i:i+chunk],\n",
    "                        N_samples,\n",
    "                        use_disp,\n",
    "                        0,\n",
    "                        0,\n",
    "                        N_importance,\n",
    "                        chunk,\n",
    "                        dataset.white_back,\n",
    "                        test_time=True)\n",
    "\n",
    "        for k, v in rendered_ray_chunks.items():\n",
    "            results[k] += [v]\n",
    "\n",
    "    for k, v in results.items():\n",
    "        results[k] = torch.cat(v, 0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "rays = sample['rays'].cuda()\n",
    "\n",
    "t = time.time()\n",
    "results = f(rays)\n",
    "torch.cuda.synchronize()\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gt = sample['rgbs'].view(img_wh[1], img_wh[0], 3)\n",
    "img_pred = results['rgb_fine'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "alpha_pred = results['opacity_fine'].view(img_wh[1], img_wh[0]).cpu().numpy()\n",
    "depth_pred = results['depth_fine'].view(img_wh[1], img_wh[0])\n",
    "\n",
    "print('PSNR', metrics.psnr(img_gt, img_pred).item())\n",
    "\n",
    "plt.subplots(figsize=(15, 8))\n",
    "plt.tight_layout()\n",
    "plt.subplot(221)\n",
    "plt.title('GT')\n",
    "plt.imshow(img_gt)\n",
    "plt.subplot(222)\n",
    "plt.title('pred')\n",
    "plt.imshow(img_pred)\n",
    "plt.subplot(223)\n",
    "plt.title('depth')\n",
    "plt.imshow(visualize_depth(depth_pred).permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_pl",
   "language": "python",
   "name": "nerf_pl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
