{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd61dcf",
   "metadata": {},
   "source": [
    "### Init and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db0ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zubairirshad/anaconda3/envs/nerf_pl/lib/python3.7/site-packages/trimesh/voxel/runlength.py:205: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  _ft = np.array([False, True], dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from models.rendering import *\n",
    "from models.nerf import *\n",
    "import metrics\n",
    "from datasets import dataset_dict\n",
    "from datasets.llff import *\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from datasets.srn_multi_ae import collate_lambda_train, collate_lambda_val\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(0)\n",
    "# torch.set_printoptions(edgeitems=20)\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# print(obj_samples)\n",
    "def contract(x, order):\n",
    "    mag = LA.norm(x, order, dim=-1)[..., None]\n",
    "    return torch.where(mag < 1, x, (2 - (1 / mag)) * (x / mag))\n",
    "\n",
    "def contract_pts(pts, radius):\n",
    "    mask = torch.norm(pts, dim=-1).unsqueeze(-1) > radius\n",
    "    new_pts = pts.clone()/radius\n",
    "    norm_pts = torch.norm(new_pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1+0.2) - 0.2/(norm_pts))*(new_pts/norm_pts)*radius\n",
    "    warped_points = mask*contracted_points + (~mask)*pts\n",
    "    return warped_points\n",
    "\n",
    "def cast_rays(t_vals, origins, directions):\n",
    "    return origins[..., None, :] + t_vals[..., None] * directions[..., None, :]\n",
    "\n",
    "\n",
    "def sample_along_rays(\n",
    "    rays_o,\n",
    "    rays_d,\n",
    "    num_samples,\n",
    "    near,\n",
    "    far,\n",
    "    randomized,\n",
    "    lindisp,\n",
    "    in_sphere,\n",
    "    far_uncontracted = 3.0\n",
    "):\n",
    "    bsz = rays_o.shape[0]\n",
    "    print(\"bsz\", bsz)\n",
    "    t_vals = torch.linspace(0.0, 1.0, num_samples + 1, device=rays_o.device)\n",
    "\n",
    "    if in_sphere:\n",
    "        if lindisp:\n",
    "            t_vals = 1.0 / (1.0 / near * (1.0 - t_vals) + 1.0 / far * t_vals)\n",
    "        else:\n",
    "            t_vals = near * (1.0 - t_vals) + far * t_vals\n",
    "            \n",
    "    else:\n",
    "        t_vals = torch.broadcast_to(t_vals, (bsz, num_samples + 1))\n",
    "\n",
    "    if randomized:\n",
    "        mids = 0.5 * (t_vals[..., 1:] + t_vals[..., :-1])\n",
    "        upper = torch.cat([mids, t_vals[..., -1:]], -1)\n",
    "        lower = torch.cat([t_vals[..., :1], mids], -1)\n",
    "        t_rand = torch.rand((bsz, num_samples + 1), device=rays_o.device)\n",
    "        t_vals = lower + (upper - lower) * t_rand\n",
    "    else:\n",
    "        t_vals = torch.broadcast_to(t_vals, (bsz, num_samples + 1))\n",
    "\n",
    "    if in_sphere:\n",
    "        coords = cast_rays(t_vals, rays_o, rays_d)\n",
    "        return t_vals, coords\n",
    "\n",
    "    else:\n",
    "        \n",
    "        t_vals_linear = far * (1.0 - t_vals) + far_uncontracted * t_vals\n",
    "        t_vals = torch.flip(\n",
    "            t_vals,\n",
    "            dims=[\n",
    "                -1,\n",
    "            ],\n",
    "        )  # 1.0 -> 0.0\n",
    "        \n",
    "        t_vals_linear = torch.flip(\n",
    "            t_vals_linear,\n",
    "            dims=[\n",
    "                -1,\n",
    "            ],\n",
    "        )  # 3.0 -> sphere \n",
    "        coords = depth2pts_outside(rays_o, rays_d, t_vals)\n",
    "        coords_linear = cast_rays(t_vals_linear, rays_o, rays_d)\n",
    "        return t_vals, coords, coords_linear\n",
    "\n",
    "\n",
    "def depth2pts_outside(rays_o, rays_d, depth):\n",
    "    \"\"\"Compute the points along the ray that are outside of the unit sphere.\n",
    "    Args:\n",
    "        rays_o: [num_rays, 3]. Ray origins of the points.\n",
    "        rays_d: [num_rays, 3]. Ray directions of the points.\n",
    "        depth: [num_rays, num_samples along ray]. Inverse of distance to sphere origin.\n",
    "    Returns:\n",
    "        pts: [num_rays, 4]. Points outside of the unit sphere. (x', y', z', 1/r)\n",
    "    \"\"\"\n",
    "    # note: d1 becomes negative if this mid point is behind camera\n",
    "    rays_o = rays_o[..., None, :].expand(\n",
    "        list(depth.shape) + [3]\n",
    "    )  #  [N_rays, num_samples, 3]\n",
    "    rays_d = rays_d[..., None, :].expand(\n",
    "        list(depth.shape) + [3]\n",
    "    )  #  [N_rays, num_samples, 3]\n",
    "    d1 = -torch.sum(rays_d * rays_o, dim=-1, keepdim=True) / torch.sum(\n",
    "        rays_d**2, dim=-1, keepdim=True\n",
    "    )\n",
    "\n",
    "    p_mid = rays_o + d1 * rays_d\n",
    "    p_mid_norm = torch.norm(p_mid, dim=-1, keepdim=True)\n",
    "    rays_d_cos = 1.0 / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\n",
    "    check_pos = 1.0 - p_mid_norm * p_mid_norm\n",
    "    assert torch.all(check_pos >= 0), \"1.0 - p_mid_norm * p_mid_norm should be greater than 0\"\n",
    "\n",
    "    d2 = torch.sqrt(1.0 - p_mid_norm * p_mid_norm) * rays_d_cos\n",
    "    p_sphere = rays_o + (d1 + d2) * rays_d\n",
    "\n",
    "    rot_axis = torch.cross(rays_o, p_sphere, dim=-1)\n",
    "    rot_axis = rot_axis / torch.norm(rot_axis, dim=-1, keepdim=True)\n",
    "    phi = torch.asin(p_mid_norm)\n",
    "    theta = torch.asin(p_mid_norm * depth[..., None])  # depth is inside [0, 1]\n",
    "    rot_angle = phi - theta  # [..., 1]\n",
    "\n",
    "    # now rotate p_sphere\n",
    "    # Rodrigues formula: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula\n",
    "    p_sphere_new = (\n",
    "        p_sphere * torch.cos(rot_angle)\n",
    "        + torch.cross(rot_axis, p_sphere, dim=-1) * torch.sin(rot_angle)\n",
    "        + rot_axis\n",
    "        * torch.sum(rot_axis * p_sphere, dim=-1, keepdim=True)\n",
    "        * (1.0 - torch.cos(rot_angle))\n",
    "    )\n",
    "    p_sphere_new = p_sphere_new / (\n",
    "        torch.norm(p_sphere_new, dim=-1, keepdim=True) + 1e-10\n",
    "    )\n",
    "    pts = torch.cat((p_sphere_new, depth.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "    return pts\n",
    "\n",
    "def get_image_coords(pixel_offset, image_height, image_width,\n",
    "):\n",
    "    \"\"\"This gets the image coordinates of one of the cameras in this object.\n",
    "    If no index is specified, it will return the maximum possible sized height / width image coordinate map,\n",
    "    by looking at the maximum height and width of all the cameras in this object.\n",
    "    Args:\n",
    "        pixel_offset: Offset for each pixel. Defaults to center of pixel (0.5)\n",
    "        index: Tuple of indices into the batch dimensions of the camera. Defaults to None, which returns the 0th\n",
    "            flattened camera\n",
    "    Returns:\n",
    "        Grid of image coordinates.\n",
    "    \"\"\"\n",
    "    image_coords = torch.meshgrid(torch.arange(image_height), torch.arange(image_width), indexing=\"ij\")\n",
    "    image_coords = torch.stack(image_coords, dim=-1) + pixel_offset  # stored as (y, x) coordinates\n",
    "    image_coords = torch.cat([image_coords, torch.ones((*image_coords.shape[:-1], 1))], dim=-1)\n",
    "    image_coords = image_coords.view(-1, 3)\n",
    "    return image_coords\n",
    "\n",
    "def get_sphere(\n",
    "    radius, center = None, color: str = \"black\", opacity: float = 1.0, resolution: int = 32\n",
    ") -> go.Mesh3d:  # type: ignore\n",
    "    \"\"\"Returns a sphere object for plotting with plotly.\n",
    "    Args:\n",
    "        radius: radius of sphere.\n",
    "        center: center of sphere. Defaults to origin.\n",
    "        color: color of sphere. Defaults to \"black\".\n",
    "        opacity: opacity of sphere. Defaults to 1.0.\n",
    "        resolution: resolution of sphere. Defaults to 32.\n",
    "    Returns:\n",
    "        sphere object.\n",
    "    \"\"\"\n",
    "    phi = torch.linspace(0, 2 * torch.pi, resolution)\n",
    "    theta = torch.linspace(-torch.pi / 2, torch.pi / 2, resolution)\n",
    "    phi, theta = torch.meshgrid(phi, theta, indexing=\"ij\")\n",
    "\n",
    "    x = torch.cos(theta) * torch.sin(phi)\n",
    "    y = torch.cos(theta) * torch.cos(phi)\n",
    "    z = torch.sin(theta)\n",
    "    pts = torch.stack((x, y, z), dim=-1)\n",
    "\n",
    "    pts *= radius\n",
    "    if center is not None:\n",
    "        pts += center\n",
    "\n",
    "    return go.Mesh3d(\n",
    "        {\n",
    "            \"x\": pts[:, :, 0].flatten(),\n",
    "            \"y\": pts[:, :, 1].flatten(),\n",
    "            \"z\": pts[:, :, 2].flatten(),\n",
    "            \"alphahull\": 0,\n",
    "            \"opacity\": opacity,\n",
    "            \"color\": color,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def vis_camera_rays(origins, directions, coords) -> go.Figure:  # type: ignore\n",
    "    \"\"\"Visualize camera rays.\n",
    "    Args:\n",
    "        camera: Camera to visualize.\n",
    "    Returns:\n",
    "        Plotly lines\n",
    "    \"\"\"\n",
    "    lines = torch.empty((origins.shape[0] * 2, 3))\n",
    "    lines[0::2] = origins\n",
    "    lines[1::2] = origins + directions*3.0\n",
    "    \n",
    "    print(\"lines\", lines.shape)\n",
    "\n",
    "    colors = torch.empty((coords.shape[0] * 2, 3))\n",
    "    colors[0::2] = coords\n",
    "    colors[1::2] = coords\n",
    "\n",
    "    data = []\n",
    "    data.append(go.Scatter3d(\n",
    "    x=lines[:, 0],\n",
    "    y=lines[:, 2],\n",
    "    z=lines[:, 1],\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=colors)))\n",
    "        \n",
    "    data.append(get_sphere(radius=1.0, color=\"#111111\", opacity=0.05))\n",
    "#     data.append(get_sphere(radius=2.0, color=\"#111111\", opacity=0.05))\n",
    "    fig = go.Figure(data = data\n",
    "        \n",
    "    )\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", showspikes=False),\n",
    "            yaxis=dict(title=\"z\", showspikes=False),\n",
    "            zaxis=dict(title=\"y\", showspikes=False),\n",
    "        ),\n",
    "        margin=dict(r=0, b=10, l=0, t=10),\n",
    "        hovermode=False,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def vis_camera_samples(all_samples) -> go.Figure:  # type: ignore\n",
    "    \"\"\"Visualize camera rays.\n",
    "    Args:\n",
    "        camera: Camera to visualize.\n",
    "    Returns:\n",
    "        Plotly lines\n",
    "    \"\"\"\n",
    "#     samples = samples.view(-1,3)\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for i in range(all_samples.shape[0]):\n",
    "        samples = all_samples[i]\n",
    "        samples_init = samples[:10, :]\n",
    "        samples_mid = samples[10:50, :]\n",
    "        samples_final = samples[50:, :]\n",
    "\n",
    "        data.append(go.Scatter3d(\n",
    "        x=samples_init[:, 0],\n",
    "        y=samples_init[:, 2],\n",
    "        z=samples_init[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, color=\"blue\")\n",
    "        ))\n",
    "\n",
    "        data.append(go.Scatter3d(\n",
    "        x=samples_mid[:, 0],\n",
    "        y=samples_mid[:, 2],\n",
    "        z=samples_mid[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, color=\"black\")\n",
    "        ))\n",
    "\n",
    "        data.append(go.Scatter3d(\n",
    "        x=samples_final[:, 0],\n",
    "        y=samples_final[:, 2],\n",
    "        z=samples_final[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, color=\"green\")\n",
    "        ))\n",
    "        \n",
    "    data.append(get_sphere(radius=1.0, color=\"#111111\", opacity=0.05))\n",
    "    data.append(get_sphere(radius=2.0, color=\"#111111\", opacity=0.05))\n",
    "    fig = go.Figure(data = data\n",
    "        \n",
    "    )\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", showspikes=False),\n",
    "            yaxis=dict(title=\"z\", showspikes=False),\n",
    "            zaxis=dict(title=\"y\", showspikes=False),\n",
    "        ),\n",
    "        margin=dict(r=0, b=10, l=0, t=10),\n",
    "        hovermode=False,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def world2camera_viewdirs(w_viewdirs, cam2world, NS):\n",
    "    w_viewdirs = repeat_interleave(w_viewdirs, NS)  # (SB*NS, B, 3)\n",
    "    rot = torch.copy(cam2world[:, :3, :3]).transpose(1, 2)  # (B, 3, 3)\n",
    "    viewdirs = torch.matmul(rot[:, None, :3, :3], w_viewdirs.unsqueeze(-1))[..., 0]\n",
    "    return viewdirs\n",
    "\n",
    "\n",
    "def world2camera(w_xyz, cam2world, NS=None):\n",
    "    \"\"\"Converts the points in world coordinates to camera view.\n",
    "    :param xyz: points in world coordinates (SB*NV, NC, 3)\n",
    "    :param poses: camera matrix (SB*NV, 4, 4)\n",
    "    :output points in camera coordinates (SB*NV, NC, 3)\n",
    "    : SB batch size\n",
    "    : NV number of views in each scene\n",
    "    : NC number of coordinate points\n",
    "    \"\"\"\n",
    "    #print(w_xyz.shape, cam2world.shape)\n",
    "    if NS is not None:\n",
    "        w_xyz = repeat_interleave(w_xyz, NS)  # (SB*NS, B, 3)\n",
    "    rot = cam2world[:, :3, :3].transpose(1, 2)  # (B, 3, 3)\n",
    "    trans = -torch.bmm(rot, cam2world[:, :3, 3:])  # (B, 3, 1)\n",
    "    #print(rot.shape, w_xyz.shape)\n",
    "    cam_rot = torch.matmul(rot[:, None, :3, :3], w_xyz.unsqueeze(-1))[..., 0]\n",
    "    cam_xyz = cam_rot + trans[:, None, :, 0]\n",
    "    # cam_xyz = cam_xyz.reshape(-1, 3)  # (SB*B, 3)\n",
    "    return cam_xyz\n",
    "\n",
    "def repeat_interleave(input, repeats, dim=0):\n",
    "    \"\"\"\n",
    "    Repeat interleave along axis 0\n",
    "    torch.repeat_interleave is currently very slow\n",
    "    https://github.com/pytorch/pytorch/issues/31980\n",
    "    \"\"\"\n",
    "    output = input.unsqueeze(1).expand(-1, repeats, *input.shape[1:])\n",
    "    return output.reshape(-1, *input.shape[1:])\n",
    "\n",
    "\n",
    "def projection(c_xyz, focal, c):\n",
    "    \"\"\"Converts [x,y,z] in camera coordinates to image coordinates \n",
    "        for the given focal length focal and image center c.\n",
    "    :param c_xyz: points in camera coordinates (SB*NV, NP, 3)\n",
    "    :param focal: focal length (SB, 2)\n",
    "    :c: image center (SB, 2)\n",
    "    :output uv: pixel coordinates (SB, NV, NP, 2)\n",
    "    \"\"\"\n",
    "    uv = -c_xyz[..., :2] / (c_xyz[..., 2:] + 1e-9)  # (SB*NV, NC, 2); NC: number of grid cells \n",
    "    uv *= repeat_interleave(\n",
    "                focal.unsqueeze(1), NV if focal.shape[0] > 1 else 1\n",
    "            )\n",
    "    uv += repeat_interleave(\n",
    "                c.unsqueeze(1), NV if c.shape[0] > 1 else 1\n",
    "            )\n",
    "    return uv\n",
    "\n",
    "\n",
    "def pos_enc(x, min_deg=0, max_deg=10):\n",
    "    scales = torch.tensor([2**i for i in range(min_deg, max_deg)]).type_as(x)\n",
    "    xb = torch.reshape((x[..., None, :] * scales[:, None]), list(x.shape[:-1]) + [-1])\n",
    "    four_feat = torch.sin(torch.cat([xb, xb + 0.5 * np.pi], dim=-1))\n",
    "    return torch.cat([x] + [four_feat], dim=-1)\n",
    "\n",
    "import open3d as o3d\n",
    "def get_world_grid(side_lengths, grid_size):\n",
    "    \"\"\" Returns a 3D grid of points in world coordinates.\n",
    "    :param side_lengths: (min, max) for each axis (3, 2)\n",
    "    :param grid_size: number of points along each dimension () or (3)\n",
    "    :output grid: (1, grid_size**3, 3)\n",
    "    \"\"\"\n",
    "    if len(grid_size) == 1:\n",
    "        grid_size = [grid_size[0] for _ in range(3)]\n",
    "        \n",
    "    w_x = torch.linspace(side_lengths[0][0], side_lengths[0][1], grid_size[0])\n",
    "    w_y = torch.linspace(side_lengths[1][0], side_lengths[1][1], grid_size[1])\n",
    "    w_z = torch.linspace(side_lengths[2][0], side_lengths[2][1], grid_size[2])\n",
    "    # Z, Y, X = torch.meshgrid(w_x, w_y, w_z)\n",
    "    X, Y, Z = torch.meshgrid(w_x, w_y, w_z)\n",
    "    w_xyz = torch.stack([X, Y, Z], axis=-1) # (gs, gs, gs, 3), gs = grid_size\n",
    "    print(w_xyz.shape)\n",
    "    w_xyz = w_xyz.reshape(-1, 3).unsqueeze(0) # (1, grid_size**3, 3)\n",
    "    return w_xyz\n",
    "\n",
    "def repeat_interleave(input, repeats, dim=0):\n",
    "    \"\"\"\n",
    "    Repeat interleave along axis 0\n",
    "    torch.repeat_interleave is currently very slow\n",
    "    https://github.com/pytorch/pytorch/issues/31980\n",
    "    \"\"\"\n",
    "    output = input.unsqueeze(1).expand(-1, repeats, *input.shape[1:])\n",
    "    return output.reshape(-1, *input.shape[1:])\n",
    "\n",
    "def w2i_projection(w_xyz, cam2world, intrinsics):\n",
    "    \"\"\"Converts the points in world coordinates to camera view.\n",
    "    :param xyz: points in world coordinates (SB*NV, NC, 3)\n",
    "    :param poses: camera matrix (SB*NV, 4, 4)\n",
    "    :output points in camera coordinates (SB*NV, NC, 3)\n",
    "    : SB batch size\n",
    "    : NV number of views in each scene\n",
    "    : NC number of coordinate points\n",
    "    \"\"\"\n",
    "    w_xyz = torch.cat([w_xyz, torch.ones_like(w_xyz[..., :1])], dim=-1)  # [n_points, 4]\n",
    "    cam_xyz = torch.inverse(cam2world).bmm(w_xyz.permute(0,2,1))\n",
    "    camera_grids = cam_xyz.permute(0,2,1)[:,:,:3]\n",
    "    projections = intrinsics[None, ...].repeat(cam2world.shape[0], 1, 1).bmm(cam_xyz[:,:3,:])\n",
    "    projections = projections.permute(0,2,1)\n",
    "    uv = projections[..., :2] / projections[..., 2:3]  # [n_views, n_points, 2]\n",
    "    return camera_grids, uv\n",
    "\n",
    "def intersect_sphere(rays_o, rays_d):\n",
    "    \"\"\"Compute the depth of the intersection point between this ray and unit sphere.\n",
    "    Args:\n",
    "        rays_o: [num_rays, 3]. Ray origins.\n",
    "        rays_d: [num_rays, 3]. Ray directions.\n",
    "    Returns:\n",
    "        depth: [num_rays, 1]. Depth of the intersection point.\n",
    "    \"\"\"\n",
    "    # note: d1 becomes negative if this mid point is behind camera\n",
    "\n",
    "    d1 = -torch.sum(rays_d * rays_o, dim=-1, keepdim=True) / torch.sum(\n",
    "        rays_d**2, dim=-1, keepdim=True\n",
    "    )\n",
    "    p = rays_o + d1 * rays_d\n",
    "    # consider the case where the ray does not intersect the sphere\n",
    "    rays_d_cos = 1.0 / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    p_norm_sq = torch.sum(p * p, dim=-1, keepdim=True)\n",
    "    check_pos = 1.0 - p_norm_sq\n",
    "    print(\"check pos\", torch.max(p_norm_sq), torch.min(p_norm_sq))\n",
    "    assert torch.all(check_pos >= 0), \"1.0 - p_norm_sq should be greater than 0\"\n",
    "    d2 = torch.sqrt(1.0 - p_norm_sq) * rays_d_cos\n",
    "    return d1 + d2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2202d69",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2b8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureNet(nn.Module):\n",
    "#     \"\"\"\n",
    "#     output 3 levels of features using a FPN structure\n",
    "#     \"\"\"\n",
    "#     def __init__(self, norm_act=nn.BatchNorm2d):\n",
    "#         super(FeatureNet, self).__init__()\n",
    "\n",
    "#         self.conv0 = nn.Sequential(\n",
    "#                         ConvBnReLU(3, 8, 3, 1, 1, norm_act=norm_act),\n",
    "#                         ConvBnReLU(8, 8, 3, 1, 1, norm_act=norm_act))\n",
    "\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#                         ConvBnReLU(8, 16, 5, 2, 2, norm_act=norm_act),\n",
    "#                         ConvBnReLU(16, 16, 3, 1, 1, norm_act=norm_act),\n",
    "#                         ConvBnReLU(16, 16, 3, 1, 1, norm_act=norm_act))\n",
    "\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#                         ConvBnReLU(16, 32, 5, 2, 2, norm_act=norm_act),\n",
    "#                         ConvBnReLU(32, 32, 3, 1, 1, norm_act=norm_act),\n",
    "#                         ConvBnReLU(32, 32, 3, 1, 1, norm_act=norm_act))\n",
    "\n",
    "#         self.toplayer = nn.Conv2d(32, 32, 1)\n",
    "#         self.latent_size = 32\n",
    "\n",
    "#     def _upsample_add(self, x, y):\n",
    "#         return F.interpolate(x, scale_factor=2,\n",
    "#                              mode=\"bilinear\", align_corners=True) + y\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B, 3, H, W)\n",
    "#         x = self.conv0(x) # (B, 8, H, W)\n",
    "#         x = self.conv1(x) # (B, 16, H//2, W//2)\n",
    "#         x = self.conv2(x) # (B, 32, H//4, W//4)\n",
    "#         x = self.toplayer(x) # (B, 32, H//4, W//4)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "# class ConvBnReLU(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels,\n",
    "#                  kernel_size=3, stride=1, pad=1,\n",
    "#                  norm_act=nn.BatchNorm2d):\n",
    "#         super(ConvBnReLU, self).__init__()\n",
    "#         self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "#                               kernel_size, stride=stride, padding=pad, bias=False)\n",
    "#         self.bn = norm_act(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "# class CostRegNet(nn.Module):\n",
    "#     def __init__(self, in_channels, norm_act=InPlaceABN):\n",
    "#         super(CostRegNet, self).__init__()\n",
    "#         self.conv0 = ConvBnReLU3D(in_channels, 8, norm_act=norm_act)\n",
    "\n",
    "#         self.conv1 = ConvBnReLU3D(8, 16, stride=2, norm_act=norm_act)\n",
    "#         self.conv2 = ConvBnReLU3D(16, 16, norm_act=norm_act)\n",
    "\n",
    "#         self.conv3 = ConvBnReLU3D(16, 32, stride=2, norm_act=norm_act)\n",
    "#         self.conv4 = ConvBnReLU3D(32, 32, norm_act=norm_act)\n",
    "\n",
    "#         self.conv5 = ConvBnReLU3D(32, 64, stride=2, norm_act=norm_act)\n",
    "#         self.conv6 = ConvBnReLU3D(64, 64, norm_act=norm_act)\n",
    "\n",
    "#         self.conv7 = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(64, 32, 3, padding=1, output_padding=1,\n",
    "#                                stride=2, bias=False),\n",
    "#             norm_act(32))\n",
    "\n",
    "#         self.conv9 = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(32, 16, 3, padding=1, output_padding=1,\n",
    "#                                stride=2, bias=False),\n",
    "#             norm_act(16))\n",
    "\n",
    "#         self.conv11 = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(16, 8, 3, padding=1, output_padding=1,\n",
    "#                                stride=2, bias=False),\n",
    "#             norm_act(8))\n",
    "\n",
    "#         # self.conv12 = nn.Conv3d(8, 8, 3, stride=1, padding=1, bias=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         conv0 = self.conv0(x)\n",
    "#         conv2 = self.conv2(self.conv1(conv0))\n",
    "#         conv4 = self.conv4(self.conv3(conv2))\n",
    "\n",
    "#         x = self.conv6(self.conv5(conv4))\n",
    "#         x = conv4 + self.conv7(x)\n",
    "#         del conv4\n",
    "#         x = conv2 + self.conv9(x)\n",
    "#         del conv2\n",
    "#         x = conv0 + self.conv11(x)\n",
    "#         del conv0\n",
    "#         # x = self.conv12(x)\n",
    "#         return x\n",
    "    \n",
    "# model = FeatureNet()\n",
    "\n",
    "# a = torch.randn((1,3,240,320))\n",
    "\n",
    "# print(model(a).shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47116a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train dataset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zubairirshad/anaconda3/envs/nerf_pl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "near torch.Size([76800, 1]) torch.Size([76800, 1])\n",
      "near torch.Size([76800, 1]) torch.Size([76800, 1])\n",
      "near torch.Size([76800, 1]) torch.Size([76800, 1])\n",
      "near torch.Size([76800, 1]) torch.Size([76800, 1])\n",
      "i 0\n",
      "src_imgs torch.Size([3, 3, 240, 320])\n",
      "src_poses torch.Size([3, 4, 4])\n",
      "src_focal torch.Size([3])\n",
      "near_obj torch.Size([4, 76800, 1])\n",
      "far_obj torch.Size([4, 76800, 1])\n",
      "instance_mask torch.Size([76800, 1])\n",
      "inst_seg_mask torch.Size([76800, 1])\n",
      "src_c torch.Size([3, 2])\n",
      "rays_o torch.Size([76800, 3])\n",
      "rays_d torch.Size([76800, 3])\n",
      "viewdirs torch.Size([76800, 3])\n",
      "target torch.Size([76800, 3])\n",
      "nocs_2d torch.Size([76800, 3])\n",
      "radii torch.Size([76800])\n",
      "multloss torch.Size([76800, 1])\n",
      "normals torch.Size([76800, 3])\n",
      "===============================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_dict['pd_multi_obj_ae_nocs']\n",
    "root_dir = '/home/zubairirshad/pd-api-py/single_scene_23'\n",
    "img_wh = (320, 240)\n",
    "\n",
    "kwargs = {'root_dir': root_dir,\n",
    "          'img_wh': tuple(img_wh),\n",
    "         'model_type': 'nerfpp',\n",
    "         'split': 'val'}\n",
    "\n",
    "train_dataset = dataset(**kwargs)\n",
    "dataloader =  DataLoader(train_dataset,\n",
    "                  shuffle=False,\n",
    "                  num_workers=0,\n",
    "                  batch_size=1,\n",
    "                  pin_memory=False)\n",
    "\n",
    "print(\"len train dataset\", len(train_dataset))\n",
    "for i, data in enumerate(dataloader):\n",
    "    print(\"i\", i)\n",
    "    for k,v in data.items():\n",
    "        print(k,v.squeeze(0).shape)\n",
    "    if i>0:\n",
    "        break\n",
    "\n",
    "    print(\"===============================\\n\\n\\n\")\n",
    "    \n",
    "for k,v in data.items():\n",
    "    data[k] = v.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66855f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nerfplusplus import helper\n",
    "near = torch.full_like(data[\"rays_o\"][..., -1:], 1e-4)\n",
    "far = helper.intersect_sphere(data[\"rays_o\"], data[\"rays_d\"])\n",
    "\n",
    "t_vals = torch.linspace(0.0, 1.0, 20 + 1)\n",
    "\n",
    "print(\"t_vals\", t_vals)\n",
    "\n",
    "t_vals = near * (1.0 - t_vals) + far * t_vals\n",
    "\n",
    "# # print(\"t_vals\", t_vals)\n",
    "\n",
    "print(\"t_vals\", t_vals[0,:])\n",
    "\n",
    "# dists = t_vals[..., 1:] - t_vals[..., :-1]\n",
    "\n",
    "# print(\"dists\", dists[:,0])\n",
    "\n",
    "# dists = torch.cat([dists, far - t_vals[..., -1:]], dim=-1)\n",
    "\n",
    "# print(\"dists\", dists.shape)\n",
    "\n",
    "\n",
    "m = (t_vals[...,1:] + t_vals[...,:-1]) * 0.5\n",
    "print(\"m\", m[0,:])\n",
    "diff = m[:, -1] - m[:, -2]\n",
    "last_val = m[:, -1] + diff\n",
    "print(\"m\", m.shape, last_val.unsqueeze(-1).shape)\n",
    "m = torch.cat([m, last_val.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "print(\"m\", m.shape, data[\"rays_d\"].shape)\n",
    "m *= torch.norm(data[\"rays_d\"][:,None,:], dim=-1)\n",
    "\n",
    "# # print(\"m\", m)\n",
    "print(\"m\", m[0,:])\n",
    "\n",
    "# print(\"t_vals\", t_vals)\n",
    "# print(\"t_vals[..., -1:]\", t_vals[..., -1:])\n",
    "# print(\"t_far - t_vals[..., -1:]\", far - t_vals[..., -1:])\n",
    "\n",
    "\n",
    "#bg\n",
    "\n",
    "# t_vals = torch.linspace(0.0, 1.0, 20 + 1)\n",
    "\n",
    "# t_vals = torch.broadcast_to(t_vals, (1000, 20 + 1))\n",
    "\n",
    "# t_vals = torch.flip(\n",
    "#     t_vals,\n",
    "#     dims=[\n",
    "#         -1,\n",
    "#     ],\n",
    "# )  # 1.0 -> 0.0\n",
    "\n",
    "# print(\"t_vals\", t_vals[0,:])\n",
    "# m = (t_vals[...,1:] + t_vals[...,:-1]) * 0.5\n",
    "# print(\"m\", m[0,:])\n",
    "# diff = m[:, -1] - m[:, -2]\n",
    "# last_val = m[:, -1] + diff\n",
    "# print(\"m\", m.shape, last_val.unsqueeze(-1).shape)\n",
    "# m = torch.cat([m, t_vals[...,-1].unsqueeze(-1)], dim=-1)\n",
    "# # m = torch.cat([m, last_val.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "# # print(\"m\", m)\n",
    "# print(\"m\", m[0,:])\n",
    "\n",
    "# print(\"t_vals[..., :-1]\", t_vals[..., :-1])\n",
    "\n",
    "# print(\"t_vals[..., 1:]\", t_vals[..., 1:])\n",
    "\n",
    "# dists = t_vals[..., :-1] - t_vals[..., 1:]\n",
    "\n",
    "# print(\"dists\", dists)\n",
    "\n",
    "# print(\"t_vals[..., :-1]\", t_vals[..., :-1])\n",
    "\n",
    "# print(\"t_vals[..., 1:]\", t_vals[..., 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b0bcd",
   "metadata": {},
   "source": [
    "### Inspect spatial encoders here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER TP MVS NEED PDMULTIOBJ AE CV dataset here\n",
    "import sys\n",
    "sys.path.append('/home/zubairirshad/nerf_pl')\n",
    "from models.nerfplusplus.encoder_tp_mvs import GridEncoder, index_grid, get_c\n",
    "encoder = GridEncoder()\n",
    "volume_feat, feats_l, depth_values = encoder(data[\"src_imgs\"], data[\"proj_mats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"volume_feat\", volume_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5830f7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torchvision resnet34 encoder\n",
      "self.pillar_aggregator_yz(latent_inp_x) torch.Size([3, 64, 64, 64, 1])\n",
      "floorplans_yz torch.Size([3, 64, 64, 512])\n",
      "grid_yz torch.Size([3, 512, 64, 64])\n",
      "===============\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([3, 256, 32, 32])\n",
      "torch.Size([3, 256, 32, 32])\n",
      "torch.Size([3, 256, 32, 32])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 16, 16])\n",
      "torch.Size([3, 128, 32, 32])\n",
      "torch.Size([3, 128, 32, 32])\n",
      "torch.Size([3, 128, 32, 32])\n",
      "torch.Size([3, 128, 32, 32])\n",
      "torch.Size([3, 128, 120, 160])\n",
      "torch.Size([3, 128, 120, 160])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zubairirshad/nerf_pl')\n",
    "from models.nerfplusplus.encoder_tp_fusion_conv import GridEncoder, index_grid, get_c\n",
    "encoder = GridEncoder()\n",
    "scene_grid_xz, scene_grid_xy, scene_grid_yz = encoder(data[\"src_imgs\"], data[\"src_poses\"], data[\"src_focal\"], data[\"src_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"scene_grid_xz\", scene_grid_xz.shape)\n",
    "# for module in encoder.modules():\n",
    "#     if isinstance(module, nn.BatchNorm2d):\n",
    "#         print(\"module\", module)\n",
    "#         module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Upsample(size=(120, 160), mode='bilinear', align_corners=True),\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"num params\", num_params)\n",
    "\n",
    "a = torch.randn((3, 512, 64, 64))\n",
    "\n",
    "b = model(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "resnet = models.resnet34(pretrained=True)\n",
    "\n",
    "# Define a new model that contains only the first 3 layers\n",
    "new_model = nn.Sequential(*list(resnet.children())[:-3])\n",
    "\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96708d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"data[src_imgs]\", data[\"src_imgs\"].shape)\n",
    "print(\"out latent\", out_latent.shape)\n",
    "print(\"out_latent2\", out_latent2.shape)\n",
    "print(\"scene_grid_xz\", scene_grid_xz.shape)\n",
    "out_latent2 = out_latent2.detach()[0]\n",
    "NV = 3\n",
    "latent = out_latent.permute(0,2,1)\n",
    "print(\"out latent\", out_latent.shape)\n",
    "out = latent.reshape(3, 96,96,96,3)\n",
    "\n",
    "for i in range(NV):\n",
    "    print(\"===============================\\n\\n\\n,\", i)\n",
    "    data_im = out[i,...]\n",
    "    print(\"data_im\", data_im.shape)\n",
    "    \n",
    "    yz = torch.mean(data_im, dim=0)\n",
    "    plt.imshow(yz.numpy())\n",
    "    plt.show()\n",
    "\n",
    "    xz = torch.mean(data_im, dim=1)\n",
    "    plt.imshow(xz.numpy())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    xy = torch.mean(data_im, dim=2)\n",
    "    print(\"xy\", xy.shape)\n",
    "    plt.imshow(xy.numpy())\n",
    "    plt.show()\n",
    "    \n",
    "print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\\n\\n\\n\")\n",
    "print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\\n\\n\\n\")\n",
    "print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\\n\\n\\n\")\n",
    "for i in range(NV):\n",
    "    print(\"===============================\\n\\n\\n,\", i)\n",
    "    data_im = out_latent2[i,...]\n",
    "    print(\"data_im\", data_im.shape)\n",
    "    \n",
    "    yz = torch.mean(data_im, dim=0)\n",
    "    plt.imshow(yz.numpy())\n",
    "    plt.show()\n",
    "\n",
    "    xz = torch.mean(data_im, dim=1)\n",
    "    plt.imshow(xz.numpy())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    xy = torch.mean(data_im, dim=2)\n",
    "    print(\"xy\", xy.shape)\n",
    "    plt.imshow(xy.numpy())\n",
    "    plt.show()\n",
    "    \n",
    "print(\"torch.min max\", torch.min(scene_grid_xz), torch.max(scene_grid_xz))\n",
    "print(\"torch.min max\", torch.min(scene_grid_yz), torch.max(scene_grid_yz))\n",
    "print(\"torch.min max\", torch.min(scene_grid_xy), torch.max(scene_grid_xy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(scene_grid_xz.squeeze(0).permute(1,2,0).detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(scene_grid_yz.squeeze(0).permute(1,2,0).detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(scene_grid_xy.squeeze(0).permute(1,2,0).detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "print(\"torch.min max\", torch.min(scene_grid_xz), torch.max(scene_grid_xz))\n",
    "print(\"torch.min max\", torch.min(scene_grid_yz), torch.max(scene_grid_yz))\n",
    "print(\"torch.min max\", torch.min(scene_grid_xy), torch.max(scene_grid_xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f4de8",
   "metadata": {},
   "source": [
    "### Get Samples and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcbe16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import models.nerfplusplus.helper as helper\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "nerfplusplus = True\n",
    "contract = False\n",
    "\n",
    "if nerfplusplus:\n",
    "    import models.nerfplusplus.helper as helper\n",
    "    from torch import linalg as LA\n",
    "    near = torch.full_like(data[\"rays_o\"][..., -1:], 1e-4)\n",
    "    far = intersect_sphere(data[\"rays_o\"], data[\"rays_d\"])\n",
    "    \n",
    "else:\n",
    "    import models.vanilla_nerf.helper as helper\n",
    "    from torch import linalg as LA\n",
    "    near = 0.2\n",
    "    far = 3.0\n",
    "\n",
    "if nerfplusplus:\n",
    "    obj_t_vals, obj_samples = sample_along_rays(\n",
    "        rays_o=data[\"rays_o\"],\n",
    "        rays_d=data[\"rays_d\"],\n",
    "        num_samples=65,\n",
    "        near = near,\n",
    "        far = far,\n",
    "        randomized=True,\n",
    "        lindisp=False,\n",
    "        in_sphere=True,\n",
    "    )\n",
    "\n",
    "    bg_t_vals, bg_samples, bg_samples_linear = sample_along_rays(\n",
    "        rays_o=data[\"rays_o\"],\n",
    "        rays_d=data[\"rays_d\"],\n",
    "        num_samples=65,\n",
    "        near=near,\n",
    "        far=far,\n",
    "        randomized=True,\n",
    "        lindisp=False,\n",
    "        in_sphere=False,\n",
    "    )\n",
    "    print(\"toch max min bg_samples linear\", torch.max(bg_samples_linear), torch.min(bg_samples_linear))\n",
    "else:\n",
    "    all_t_vals, all_samples = helper.sample_along_rays(\n",
    "        rays_o=data[\"rays_o\"],\n",
    "        rays_d=data[\"rays_d\"],\n",
    "        num_samples=65,\n",
    "        near=near,\n",
    "        far=far,\n",
    "        randomized=True,\n",
    "        lindisp=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "def reverse_contract_pts(pts, radius):\n",
    "    norm_pts = torch.norm(pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1 + 0.2) - 0.2 / norm_pts) * (pts / norm_pts) * radius\n",
    "    return contracted_points\n",
    "\n",
    "def contract_pts(pts, radius=3):\n",
    "    mask = torch.norm(pts, dim=-1).unsqueeze(-1) > radius\n",
    "    new_pts = pts.clone()/radius\n",
    "    norm_pts = torch.norm(new_pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1+0.5) - 0.5/(norm_pts))*(new_pts/norm_pts)*radius\n",
    "    warped_points = torch.where(\n",
    "        mask == False, pts, mask*contracted_points \n",
    "    )\n",
    "    \n",
    "    return warped_points\n",
    "\n",
    "def uncontract_pts(pts, radius=1):\n",
    "    mask = torch.norm(pts, dim=-1).unsqueeze(-1) > radius\n",
    "    new_pts = pts.clone()/radius\n",
    "    norm_pts = torch.norm(new_pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1+0.6) - 0.6/(norm_pts))*(new_pts/norm_pts)*radius\n",
    "    warped_points = torch.where(\n",
    "        mask == False, pts, mask*contracted_points \n",
    "    )\n",
    "    \n",
    "    return warped_points\n",
    "\n",
    "def contract_samples(x, order=2):\n",
    "    mag = LA.norm(x, order, dim=-1)[..., None]\n",
    "    return torch.where(mag < 1, x, (2 - (1 / mag)) * (x / mag))\n",
    "\n",
    "# def inverse_contract_samples(x, order = order):\n",
    "#     mag = torch.linalg.norm(x, ord=order, dim=-1)[..., None]\n",
    "#     mask = mag < 1\n",
    "#     expanded = x * mag*mag / (2 - (1 / mag))\n",
    "#     return torch.where(mask, x, expanded)\n",
    "\n",
    "def _contract(x):\n",
    "    x_mag_sq = torch.sum(x**2, dim=-1, keepdim=True).clip(min=1e-32)\n",
    "    z = torch.where(\n",
    "        x_mag_sq <= 1, x, ((2 * torch.sqrt(x_mag_sq) - 1) / x_mag_sq) * x\n",
    "    )\n",
    "    return z\n",
    "\n",
    "def _inverse_contract(x):\n",
    "    x_mag_sq = torch.sum(x**2, dim=-1, keepdim=True).clip(min=1e-32)\n",
    "    z = torch.where(\n",
    "        x_mag_sq <= 1, x, x * (x_mag_sq / (2 * torch.sqrt(x_mag_sq) - 1))\n",
    "    )\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "if nerfplusplus:\n",
    "#     all_samples = torch.cat((obj_samples, bg_samples[:,:,:3]), dim=0)\n",
    "#     all_samples = torch.cat((obj_samples, bg_samples_linear), dim=0)\n",
    "\n",
    "    all_samples = obj_samples\n",
    "    print(\"all_samples\", all_samples.shape)\n",
    "else:\n",
    "    if contract:\n",
    "        all_samples = contract_samples(all_samples, float('inf'))\n",
    "        print(\"torch min max  all samples\", torch.min(samples), torch.max(samples))\n",
    "#         all_samples = _inverse_contract(all_samples)\n",
    "\n",
    "\n",
    "if nerfplusplus:\n",
    "    print(\"bg obj samples\",bg_samples.shape, obj_samples.shape)\n",
    "else:\n",
    "    print(\"all_samples\",all_samples.shape)\n",
    "\n",
    "\n",
    "coords = get_image_coords(pixel_offset = 0.5,image_height = 120, image_width = 160)\n",
    "print(coords.shape)\n",
    "\n",
    "rays_o = data[\"rays_o\"][:20,:]\n",
    "rays_d = data[\"rays_d\"][:20,:]\n",
    "coords = coords[:20,:]\n",
    "\n",
    "if nerfplusplus:\n",
    "    num = np.random.choice(all_samples.shape[0], 20)\n",
    "    samples = all_samples[num,:, :3]\n",
    "    \n",
    "#     num_bg = np.random.choice(bg_samples.shape[0], 20)\n",
    "    samples_bg = bg_samples[num,:, :3]\n",
    "    \n",
    "    print(\"inverse radius \", bg_samples[:,:,3])\n",
    "    \n",
    "#     num_bg_linear = np.random.choice(bg_samples.shape[0], 20)\n",
    "    samples_bg_linear = bg_samples_linear[num,:, :3]\n",
    "#     print(samples.shape, samples_bg.shape)\n",
    "    samples = torch.cat((samples, samples_bg, samples_bg_linear), dim=0)\n",
    "#     samples = samples_bg_linear\n",
    "else:\n",
    "    num = np.random.choice(all_samples.shape[0], 20)\n",
    "    samples = all_samples[num,:, :3]\n",
    "\n",
    "\n",
    "fig = vis_camera_samples(samples)\n",
    "fig.show()\n",
    "\n",
    "print(\"torch min max samples\", torch.min(samples), torch.max(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all_samples\", all_samples.shape)\n",
    "\n",
    "samples_w = all_samples.reshape(-1,3).unsqueeze(0)\n",
    "samples_cam = world2camera(samples_w, data[\"src_poses\"], 3)\n",
    "\n",
    "print(\"samples\",samples_cam.shape)\n",
    "\n",
    "print(\"all_samples\", torch.min(all_samples), torch.max(all_samples))\n",
    "scale_factor = 1\n",
    "uv_xz = samples_cam[:, :, [0, 2]].float()\n",
    "uv_xz = (uv_xz/scale_factor).unsqueeze(2)\n",
    "\n",
    "print(\"uv_xz\", uv_xz.shape)\n",
    "\n",
    "grid = torch.randn((3, 128, 64, 64))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "scene_latent_xz = F.grid_sample(grid, uv_xz, align_corners=True, mode='bilinear', padding_mode='zeros')\n",
    "\n",
    "print(\"scene_latent_xz\", scene_latent_xz.shape)\n",
    "\n",
    "scene_latent_xy = scene_latent_xz\n",
    "\n",
    "scene_latent_yz = scene_latent_xz\n",
    "\n",
    "output = torch.sum(torch.stack([scene_latent_xz, scene_latent_xy, scene_latent_yz]), dim=0)\n",
    "print(output[..., 0].shape)\n",
    "\n",
    "print(\"output[..., 0].transpose(1,2)\", output[..., 0].permute(0,2, 1).shape)\n",
    "output = output[..., 0].permute(0,2, 1).reshape(-1, output.shape[1])\n",
    "print(output.shape)\n",
    "# a = torch.randn((2000,65,3))\n",
    "# uv_xz = a[:, :, [0, 2]].reshape(-1,2).unsqueeze(0).float()\n",
    "\n",
    "# print(uv_xz.shape)\n",
    "\n",
    "\n",
    "# print(\"a[:,:,0].float().unsqueeze(-1).\", a[:,:,0].float().unsqueeze(-1).shape)\n",
    "# x = a[:,:,0].float().unsqueeze(-1).reshape(-1,1)\n",
    "# y = a[:,:,1].float().unsqueeze(-1).reshape(-1,1)\n",
    "\n",
    "# print(\"x before \", x)\n",
    "\n",
    "# x = x*-1\n",
    "# y=y*-1\n",
    "\n",
    "# print(\"x after\", x)\n",
    "\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "# uv_xz = torch.stack([x, y], dim=-1)\n",
    "# print(uv_xz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e520c5",
   "metadata": {},
   "source": [
    "\n",
    "### Encode for PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d379c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from models.vanilla_nerf.encoder import *\n",
    "NV = 1\n",
    "\n",
    "def unprocess_images(normalized_images, shape = ()):\n",
    "    inverse_transform = T.Compose([T.Normalize((-0.5/0.5, -0.5/0.5, -0.5/0.5), (1/0.5, 1/0.5, 1/0.5))])\n",
    "    print(\"unnormalize dimgs\", inverse_transform(normalized_images).shape)\n",
    "    return inverse_transform(normalized_images)\n",
    "\n",
    "w_xyz = all_samples[:,:,:3]\n",
    "w_xyz = w_xyz.reshape(-1,3).unsqueeze(0)\n",
    "# print(\"wxyz, poses\", w_xyz.shape, poses.shape)\n",
    "poses = torch.clone(data[\"src_poses\"])\n",
    "cam_xyz = world2camera(w_xyz, poses, NV)\n",
    "\n",
    "print(\"cam_xyz\", cam_xyz.shape)\n",
    "\n",
    "\n",
    "encoder = SpatialEncoder(backbone=\"resnet34\",\n",
    "                                              pretrained=True,\n",
    "                                              num_layers=4,\n",
    "                                              index_interp=\"bilinear\",\n",
    "                                              index_padding=\"zeros\",\n",
    "                                              # index_padding=\"border\",\n",
    "                                              upsample_interp=\"bilinear\",\n",
    "                                              feature_scale=1.0,\n",
    "                                              use_first_pool=True,\n",
    "                                              norm_type=\"batch\")\n",
    "\n",
    "latent = encoder(data[\"src_imgs\"])\n",
    "print(latent.shape)\n",
    "# height, width = latent.size()[2:]\n",
    "\n",
    "\n",
    "\n",
    "# after encoder unnormalize images\n",
    "\n",
    "for i in range(NV):\n",
    "    plt.imshow(data[\"src_imgs\"][i].permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "    \n",
    "new_images = unprocess_images(data[\"src_imgs\"])\n",
    "\n",
    "for i in range(NV):\n",
    "    plt.imshow(new_images[i].permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "\n",
    "height, width = new_images.size()[2:]\n",
    "\n",
    "#PN projection\n",
    "focal = data[\"src_focal\"][0].unsqueeze(-1).repeat((1, 2))\n",
    "focal[..., 1] *= -1.0\n",
    "c = data[\"src_c\"][0].unsqueeze(0)\n",
    "uv_pn = projection(cam_xyz, focal, c)\n",
    "\n",
    "print(\"focal, c\", focal, c)\n",
    "\n",
    "im_x = uv_pn[:,:, 0]\n",
    "im_y = uv_pn[:,:, 1]\n",
    "im_grid_pn = torch.stack([2 * im_x / (width - 1) - 1, 2 * im_y / (height - 1) - 1], dim=-1)\n",
    "\n",
    "print(\"torch min max im_grid_pn\", torch.min(im_grid_pn), torch.max(im_grid_pn))\n",
    "\n",
    "#K [R|T\\ projection\n",
    "# focal = data[\"src_focal\"]\n",
    "# c = data[\"src_c\"]\n",
    "\n",
    "# print(\"data src focl, src c\",\"data[src_focal]\", data[\"src_focal\"], data[\"src_c\"])\n",
    "# K = torch.FloatTensor([\n",
    "#     [data[\"src_focal\"][0], 0., data[\"src_c\"][0][0]],\n",
    "#     [0., data[\"src_focal\"][0], data[\"src_c\"][0][1]],\n",
    "#     [0., 0., 1.],\n",
    "# ])\n",
    "# print(\"K\", K)\n",
    "\n",
    "# poses = data[\"src_poses\"]\n",
    "# world_xyz = repeat_interleave(w_xyz, NV)  # (SB*NS, B, 3)\n",
    "# cam_xyz, uv_rt= w2i_projection(world_xyz, poses, K)\n",
    "# image_points_rt = uv_rt[0,:,:]\n",
    "\n",
    "# width = 320\n",
    "# height = 240\n",
    "# im_x = uv_rt[:,:, 0]\n",
    "# im_y = uv_rt[:,:, 1]\n",
    "# im_grid_rt = torch.stack([2 * im_x / (width - 1) - 1, 2 * im_y / (height - 1) - 1], dim=-1)\n",
    "\n",
    "# inbound_rt = torch.logical_and(np.logical_and(image_points_rt[:, 0] > 0, image_points_rt[:, 0] < width),\n",
    "#                       np.logical_and(image_points_rt[:, 1] > 0, image_points_rt[:, 1] < height))\n",
    "# print(\"inbound uv\",(inbound_rt ==True).sum()) \n",
    "\n",
    "\n",
    "# image_points_pn = uv_pn[0,:,:]\n",
    "\n",
    "# inbound_pn = torch.logical_and(np.logical_and(image_points_pn[:, 0] > 0, image_points_pn[:, 0] < width),\n",
    "#                       np.logical_and(image_points_pn[:, 1] > 0, image_points_pn[:, 1] < height))\n",
    "\n",
    "# inbound_bool_pn = inbound_pn.bool()\n",
    "# za_inbound_pn = (inbound_bool_pn ==True).sum()\n",
    "# print(\"za\",za_inbound_pn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec178e5d",
   "metadata": {},
   "source": [
    "### Plot sampled projected pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb9de4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"data[src_imgs]\", torch.min(new_images[0]), torch.max(new_images[0]))\n",
    "print(\"data[src_imgs]\", new_images.shape)\n",
    "for i in range(NV):\n",
    "    plt.imshow(new_images[i].permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "\n",
    "imgs = new_images\n",
    "grid = im_grid_pn\n",
    "grid = grid.unsqueeze(2)\n",
    "\n",
    "mask = grid.abs() <= 1\n",
    "za_inbound_mask = (mask ==True).sum()\n",
    "print(\"za_inbound_mask\", za_inbound_mask, (za_inbound_mask/(grid.shape[0]*grid.shape[1]*grid.shape[-1]))*100)\n",
    "#sampled_images = F.grid_sample(imgs, grid, align_corners=True, mode='bilinear', padding_mode=\"zeros\")\n",
    "\n",
    "V = NV\n",
    "C=3\n",
    "colors = torch.empty((grid.shape[1], V*C), device=imgs.device, dtype=torch.float)\n",
    "print(\"colors\", colors.shape)\n",
    "for i, idx in enumerate(range(imgs.shape[0])):\n",
    "    print(\"imgs[idx, :, :, :].unsqueeze(0)\", imgs[idx, :, :, :].unsqueeze(0).shape)\n",
    "    print(\"grid[idx, :, :].unsqueeze(0)\", grid[idx, :, :].unsqueeze(0).shape)\n",
    "    data_im = F.grid_sample(imgs[idx, :, :, :].unsqueeze(0), grid[idx, :, :].unsqueeze(0), align_corners=True, mode='bilinear', padding_mode='zeros')\n",
    "    print(\"data\", data_im.shape)\n",
    "\n",
    "    # Vis\n",
    "    print(\"data_im[0].permute(1, 2, 0)\", data_im[0].squeeze(-1).permute(1,0).shape)\n",
    "    colors[...,i*C:i*C+C] = data_im[0].squeeze(-1).permute(1,0)\n",
    "    all_imgs = data_im.squeeze(-1).squeeze(0).permute(1,0).reshape(240,320,65,3).numpy()\n",
    "    img_1 = all_imgs[:,:,10,:]\n",
    "    plt.imshow(img_1)\n",
    "    plt.show()\n",
    "\n",
    "# print(\"samples image >0\", ((sampled_images>0) ==True).sum())\n",
    "# print(\"sampled_images\", sampled_images.shape)\n",
    "\n",
    "# for i in range(sampled_images.shape[0]):\n",
    "#     if i!=1:\n",
    "#         continue\n",
    "#     all_imgs = sampled_images[i].squeeze(-1).permute(1,0).reshape(240,320,65, 3).numpy()\n",
    "#     img_1 = all_imgs[:,:,50,:]\n",
    "#     plt.imshow(img_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afcbd2b",
   "metadata": {},
   "source": [
    "### Get world grid and define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_length = 1.0\n",
    "grid_size=[256, 256, 256]\n",
    "sfactor=8\n",
    "\n",
    "world_grid = get_world_grid([[-side_length, side_length],\n",
    "                                       [-side_length, side_length],\n",
    "                                       [0, side_length],\n",
    "                                       ], [int(grid_size[0]/sfactor), int(grid_size[1]/sfactor), int(grid_size[2]/sfactor)] )  # (1, grid_size**3, 3)\n",
    "\n",
    "print(world_grid.shape)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(world_grid.squeeze(0).numpy())\n",
    "\n",
    "o3d.visualization.draw_plotly([pcd])\n",
    "\n",
    "world_grids = repeat_interleave(world_grid.clone(),\n",
    "                                          1 * 1 * 3) # (SB*NV, NC, 3) NC: number of grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(world_grids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69f843",
   "metadata": {},
   "source": [
    "### Get uv projection and K*[R\\T] projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = latent.size()[2:]\n",
    "focal = data[\"src_focal\"]/2\n",
    "# focal[..., 1] *= -1.0\n",
    "c = data[\"src_c\"]/2\n",
    "K = torch.FloatTensor([\n",
    "    [data[\"src_focal\"][0], 0., data[\"src_c\"][0][0]],\n",
    "    [0., data[\"src_focal\"][1], data[\"src_c\"][0][1]],\n",
    "    [0., 0., 1.],\n",
    "])\n",
    "\n",
    "poses = data[\"src_poses\"]\n",
    "\n",
    "\n",
    "cam_xyz, uv = w2i_projection(world_grids, poses, K)\n",
    "image_points = uv[1,:,:]\n",
    "height, width = latent.size()[2:]\n",
    "print(height, width)\n",
    "inbound = torch.logical_and(np.logical_and(image_points[:, 0] > 0, image_points[:, 0] < width),\n",
    "                      np.logical_and(image_points[:, 1] > 0, image_points[:, 1] < height))\n",
    "print(\"inbound uv\",(inbound ==True).sum()) \n",
    "\n",
    "\n",
    "camera_grids_w2c = world2camera(world_grids, poses)\n",
    "focal_uv = data[\"src_focal\"][0].unsqueeze(-1).repeat((1, 2))\n",
    "focal_uv[..., 1] *= -1.0\n",
    "c_uv = data[\"src_c\"][0].unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_points = uv_projection[1,:,:]\n",
    "inbound = torch.logical_and(np.logical_and(image_points[:, 0] > 0, image_points[:, 0] < width),\n",
    "                      np.logical_and(image_points[:, 1] > 0, image_points[:, 1] < height))\n",
    "\n",
    "print(\"inbound projection\",(inbound ==True).sum()) \n",
    "\n",
    "diff = torch.norm(camera_grids_w2c- cam_xyz)\n",
    "print(\"diff\",diff)\n",
    "\n",
    "diff_uv = torch.norm(uv_projection- uv)\n",
    "\n",
    "print(\"uv min\", torch.min(uv), torch.max(uv))\n",
    "print(\"uv proj min\", torch.min(uv_projection), torch.max(uv_projection))\n",
    "\n",
    "print(\"diff uv\", diff_uv)\n",
    "\n",
    "print(\"uv\", uv)\n",
    "print(\"uv_projection\", uv_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604852b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "latent = torch.randn((1,3, 96,96,96,128))\n",
    "print(latent.shape)\n",
    "\n",
    "latent = latent.mean(1)\n",
    "\n",
    "print(latent.shape)\n",
    "\n",
    "latent = latent.permute(0,4,1,2,3)\n",
    "\n",
    "print(latent.shape)\n",
    "\n",
    "feature = torch.randn((1, 8, 96, 96, 96))\n",
    "\n",
    "print(feature.shape)\n",
    "\n",
    "samples = torch.randn((2000,65,3))\n",
    "\n",
    "samples = samples.view(-1,3)\n",
    "\n",
    "samples = samples.view(1, 1, 1, -1, 3) * 2 - 1.0  # [1 1 H W 3] (x,y,z)\n",
    "print(samples.shape)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "data_im = F.grid_sample(feature, samples, align_corners=True, mode='bilinear')\n",
    "\n",
    "print(\"data im\", data_im.shape)\n",
    "\n",
    "out = data_im.squeeze().permute(1,0)\n",
    "\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cd0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "t_vals = torch.linspace(0.0, 1.0, 64 + 1)\n",
    "\n",
    "print(\"t_vals\", t_vals)\n",
    "\n",
    "near = 0.3\n",
    "\n",
    "far = 3.0\n",
    "\n",
    "t_vals = near * (1.0 - t_vals) + far * t_vals\n",
    "\n",
    "print(\"t_vals\", t_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "img = torch.randn((3,3,240,320))\n",
    "a = torch.Tensor([img.shape[-1], img.shape[-2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a/2\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 'compact_mini_01_body-168.png'\n",
    "\n",
    "# print()\n",
    "num_str = str(12)\n",
    "print(num_str.zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/zubairirshad/pd-api-py/PDMultiObjv6/train'\n",
    "import os\n",
    "import shutil\n",
    "# Loop through all the subfolders in the path\n",
    "for foldername in os.listdir(path):\n",
    "#     if foldername!='SF_VanNessAveAndTurkSt4':\n",
    "#         continue\n",
    "    # Construct the path to the \"nocs_2d\" folder\n",
    "    nocs_path = os.path.join(path, foldername, 'train', 'instance_masks_2d')\n",
    "    # Loop through all the images from 0 to 198\n",
    "    filename = os.listdir(nocs_path)[0]\n",
    "    filename = filename.split('-')[0]\n",
    "    for i in range(199):\n",
    "        num_str = str(i)\n",
    "        # Construct the filename for the image\n",
    "        # Check if the file exists in the folder\n",
    "        name = filename + '-'+num_str.zfill(3) +'.png'\n",
    "        if not os.path.exists(os.path.join(nocs_path, name)):\n",
    "            \n",
    "            num_last = str(i-1)\n",
    "            last_name = filename + '-'+num_last.zfill(3) +'.png'\n",
    "            last_file_path = os.path.join(nocs_path, last_name)\n",
    "            \n",
    "            file_path = os.path.join(nocs_path, name)\n",
    "            # copy the last file and rename it with the current index\n",
    "            shutil.copy2(last_file_path, file_path)\n",
    "#             os.rename(file_path, os.path.join(folder_path, file_name))\n",
    "            \n",
    "            print(f'Missing file in folder {foldername}: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '3view_LPIPS'\n",
    "\n",
    "if 'LPIPS' in a:\n",
    "    print(\"TRUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c209d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_pl",
   "language": "python",
   "name": "nerf_pl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
