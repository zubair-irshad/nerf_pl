{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd61dcf",
   "metadata": {},
   "source": [
    "### Init and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db0ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zubairirshad/anaconda3/envs/nerf_pl/lib/python3.7/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping\n",
      "/home/zubairirshad/anaconda3/envs/nerf_pl/lib/python3.7/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, Set, Iterable\n",
      "/home/zubairirshad/anaconda3/envs/nerf_pl/lib/python3.7/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, Set, Iterable\n",
      "/home/zubairirshad/anaconda3/envs/nerf_pl/lib/python3.7/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.int, \"int\"), (np.int8, \"int\"),\n",
      "/home/zubairirshad/anaconda3/envs/nerf_pl/lib/python3.7/site-packages/trimesh/voxel/runlength.py:205: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  _ft = np.array([False, True], dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from models.rendering import *\n",
    "from models.nerf import *\n",
    "import metrics\n",
    "from datasets import dataset_dict\n",
    "from datasets.llff import *\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from datasets.srn_multi_ae import collate_lambda_train, collate_lambda_val\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(0)\n",
    "# torch.set_printoptions(edgeitems=20)\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# print(obj_samples)\n",
    "def contract(x, order):\n",
    "    mag = LA.norm(x, order, dim=-1)[..., None]\n",
    "    return torch.where(mag < 1, x, (2 - (1 / mag)) * (x / mag))\n",
    "\n",
    "def contract_pts(pts, radius):\n",
    "    mask = torch.norm(pts, dim=-1).unsqueeze(-1) > radius\n",
    "    new_pts = pts.clone()/radius\n",
    "    norm_pts = torch.norm(new_pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1+0.2) - 0.2/(norm_pts))*(new_pts/norm_pts)*radius\n",
    "    warped_points = mask*contracted_points + (~mask)*pts\n",
    "    return warped_points\n",
    "\n",
    "def cast_rays(t_vals, origins, directions):\n",
    "    return origins[..., None, :] + t_vals[..., None] * directions[..., None, :]\n",
    "\n",
    "def convert_pose(C2W):\n",
    "    flip_yz = np.eye(4)\n",
    "    flip_yz[1, 1] = -1\n",
    "    flip_yz[2, 2] = -1\n",
    "    C2W = np.matmul(C2W, flip_yz)\n",
    "    return C2W\n",
    "\n",
    "def sample_along_rays(\n",
    "    rays_o,\n",
    "    rays_d,\n",
    "    num_samples,\n",
    "    near,\n",
    "    far,\n",
    "    randomized,\n",
    "    lindisp,\n",
    "    in_sphere,\n",
    "    far_uncontracted = 3.0\n",
    "):\n",
    "    bsz = rays_o.shape[0]\n",
    "    print(\"bsz\", bsz)\n",
    "    t_vals = torch.linspace(0.0, 1.0, num_samples + 1, device=rays_o.device)\n",
    "\n",
    "    if in_sphere:\n",
    "        if lindisp:\n",
    "            t_vals = 1.0 / (1.0 / near * (1.0 - t_vals) + 1.0 / far * t_vals)\n",
    "        else:\n",
    "            t_vals = near * (1.0 - t_vals) + far * t_vals\n",
    "            \n",
    "    else:\n",
    "        t_vals = torch.broadcast_to(t_vals, (bsz, num_samples + 1))\n",
    "\n",
    "    if randomized:\n",
    "        mids = 0.5 * (t_vals[..., 1:] + t_vals[..., :-1])\n",
    "        upper = torch.cat([mids, t_vals[..., -1:]], -1)\n",
    "        lower = torch.cat([t_vals[..., :1], mids], -1)\n",
    "        t_rand = torch.rand((bsz, num_samples + 1), device=rays_o.device)\n",
    "        t_vals = lower + (upper - lower) * t_rand\n",
    "    else:\n",
    "        t_vals = torch.broadcast_to(t_vals, (bsz, num_samples + 1))\n",
    "\n",
    "    if in_sphere:\n",
    "        coords = cast_rays(t_vals, rays_o, rays_d)\n",
    "        return t_vals, coords\n",
    "\n",
    "    else:\n",
    "        \n",
    "        t_vals_linear = far * (1.0 - t_vals) + far_uncontracted * t_vals\n",
    "        t_vals = torch.flip(\n",
    "            t_vals,\n",
    "            dims=[\n",
    "                -1,\n",
    "            ],\n",
    "        )  # 1.0 -> 0.0\n",
    "        \n",
    "        t_vals_linear = torch.flip(\n",
    "            t_vals_linear,\n",
    "            dims=[\n",
    "                -1,\n",
    "            ],\n",
    "        )  # 3.0 -> sphere \n",
    "        coords = depth2pts_outside(rays_o, rays_d, t_vals)\n",
    "        coords_linear = cast_rays(t_vals_linear, rays_o, rays_d)\n",
    "        return t_vals, coords, coords_linear\n",
    "\n",
    "\n",
    "def depth2pts_outside(rays_o, rays_d, depth):\n",
    "    \"\"\"Compute the points along the ray that are outside of the unit sphere.\n",
    "    Args:\n",
    "        rays_o: [num_rays, 3]. Ray origins of the points.\n",
    "        rays_d: [num_rays, 3]. Ray directions of the points.\n",
    "        depth: [num_rays, num_samples along ray]. Inverse of distance to sphere origin.\n",
    "    Returns:\n",
    "        pts: [num_rays, 4]. Points outside of the unit sphere. (x', y', z', 1/r)\n",
    "    \"\"\"\n",
    "    # note: d1 becomes negative if this mid point is behind camera\n",
    "    rays_o = rays_o[..., None, :].expand(\n",
    "        list(depth.shape) + [3]\n",
    "    )  #  [N_rays, num_samples, 3]\n",
    "    rays_d = rays_d[..., None, :].expand(\n",
    "        list(depth.shape) + [3]\n",
    "    )  #  [N_rays, num_samples, 3]\n",
    "    d1 = -torch.sum(rays_d * rays_o, dim=-1, keepdim=True) / torch.sum(\n",
    "        rays_d**2, dim=-1, keepdim=True\n",
    "    )\n",
    "\n",
    "    p_mid = rays_o + d1 * rays_d\n",
    "    p_mid_norm = torch.norm(p_mid, dim=-1, keepdim=True)\n",
    "    rays_d_cos = 1.0 / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "\n",
    "    check_pos = 1.0 - p_mid_norm * p_mid_norm\n",
    "    assert torch.all(check_pos >= 0), \"1.0 - p_mid_norm * p_mid_norm should be greater than 0\"\n",
    "\n",
    "    d2 = torch.sqrt(1.0 - p_mid_norm * p_mid_norm) * rays_d_cos\n",
    "    p_sphere = rays_o + (d1 + d2) * rays_d\n",
    "\n",
    "    rot_axis = torch.cross(rays_o, p_sphere, dim=-1)\n",
    "    rot_axis = rot_axis / torch.norm(rot_axis, dim=-1, keepdim=True)\n",
    "    phi = torch.asin(p_mid_norm)\n",
    "    theta = torch.asin(p_mid_norm * depth[..., None])  # depth is inside [0, 1]\n",
    "    rot_angle = phi - theta  # [..., 1]\n",
    "\n",
    "    # now rotate p_sphere\n",
    "    # Rodrigues formula: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula\n",
    "    p_sphere_new = (\n",
    "        p_sphere * torch.cos(rot_angle)\n",
    "        + torch.cross(rot_axis, p_sphere, dim=-1) * torch.sin(rot_angle)\n",
    "        + rot_axis\n",
    "        * torch.sum(rot_axis * p_sphere, dim=-1, keepdim=True)\n",
    "        * (1.0 - torch.cos(rot_angle))\n",
    "    )\n",
    "    p_sphere_new = p_sphere_new / (\n",
    "        torch.norm(p_sphere_new, dim=-1, keepdim=True) + 1e-10\n",
    "    )\n",
    "    pts = torch.cat((p_sphere_new, depth.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "    return pts\n",
    "\n",
    "def get_image_coords(pixel_offset, image_height, image_width,\n",
    "):\n",
    "    \"\"\"This gets the image coordinates of one of the cameras in this object.\n",
    "    If no index is specified, it will return the maximum possible sized height / width image coordinate map,\n",
    "    by looking at the maximum height and width of all the cameras in this object.\n",
    "    Args:\n",
    "        pixel_offset: Offset for each pixel. Defaults to center of pixel (0.5)\n",
    "        index: Tuple of indices into the batch dimensions of the camera. Defaults to None, which returns the 0th\n",
    "            flattened camera\n",
    "    Returns:\n",
    "        Grid of image coordinates.\n",
    "    \"\"\"\n",
    "    image_coords = torch.meshgrid(torch.arange(image_height), torch.arange(image_width), indexing=\"ij\")\n",
    "    image_coords = torch.stack(image_coords, dim=-1) + pixel_offset  # stored as (y, x) coordinates\n",
    "    image_coords = torch.cat([image_coords, torch.ones((*image_coords.shape[:-1], 1))], dim=-1)\n",
    "    image_coords = image_coords.view(-1, 3)\n",
    "    return image_coords\n",
    "\n",
    "def get_sphere(\n",
    "    radius, center = None, color: str = \"black\", opacity: float = 1.0, resolution: int = 32\n",
    ") -> go.Mesh3d:  # type: ignore\n",
    "    \"\"\"Returns a sphere object for plotting with plotly.\n",
    "    Args:\n",
    "        radius: radius of sphere.\n",
    "        center: center of sphere. Defaults to origin.\n",
    "        color: color of sphere. Defaults to \"black\".\n",
    "        opacity: opacity of sphere. Defaults to 1.0.\n",
    "        resolution: resolution of sphere. Defaults to 32.\n",
    "    Returns:\n",
    "        sphere object.\n",
    "    \"\"\"\n",
    "    phi = torch.linspace(0, 2 * torch.pi, resolution)\n",
    "    theta = torch.linspace(-torch.pi / 2, torch.pi / 2, resolution)\n",
    "    phi, theta = torch.meshgrid(phi, theta, indexing=\"ij\")\n",
    "\n",
    "    x = torch.cos(theta) * torch.sin(phi)\n",
    "    y = torch.cos(theta) * torch.cos(phi)\n",
    "    z = torch.sin(theta)\n",
    "    pts = torch.stack((x, y, z), dim=-1)\n",
    "\n",
    "    pts *= radius\n",
    "    if center is not None:\n",
    "        pts += center\n",
    "\n",
    "    return go.Mesh3d(\n",
    "        {\n",
    "            \"x\": pts[:, :, 0].flatten(),\n",
    "            \"y\": pts[:, :, 1].flatten(),\n",
    "            \"z\": pts[:, :, 2].flatten(),\n",
    "            \"alphahull\": 0,\n",
    "            \"opacity\": opacity,\n",
    "            \"color\": color,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def vis_camera_rays(origins, directions, coords) -> go.Figure:  # type: ignore\n",
    "    \"\"\"Visualize camera rays.\n",
    "    Args:\n",
    "        camera: Camera to visualize.\n",
    "    Returns:\n",
    "        Plotly lines\n",
    "    \"\"\"\n",
    "    lines = torch.empty((origins.shape[0] * 2, 3))\n",
    "    lines[0::2] = origins\n",
    "    lines[1::2] = origins + directions*3.0\n",
    "    \n",
    "    print(\"lines\", lines.shape)\n",
    "\n",
    "    colors = torch.empty((coords.shape[0] * 2, 3))\n",
    "    colors[0::2] = coords\n",
    "    colors[1::2] = coords\n",
    "\n",
    "    data = []\n",
    "    data.append(go.Scatter3d(\n",
    "    x=lines[:, 0],\n",
    "    y=lines[:, 2],\n",
    "    z=lines[:, 1],\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=colors)))\n",
    "        \n",
    "    data.append(get_sphere(radius=1.0, color=\"#111111\", opacity=0.05))\n",
    "#     data.append(get_sphere(radius=2.0, color=\"#111111\", opacity=0.05))\n",
    "    fig = go.Figure(data = data\n",
    "        \n",
    "    )\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", showspikes=False),\n",
    "            yaxis=dict(title=\"z\", showspikes=False),\n",
    "            zaxis=dict(title=\"y\", showspikes=False),\n",
    "        ),\n",
    "        margin=dict(r=0, b=10, l=0, t=10),\n",
    "        hovermode=False,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def vis_camera_samples(all_samples) -> go.Figure:  # type: ignore\n",
    "    \"\"\"Visualize camera rays.\n",
    "    Args:\n",
    "        camera: Camera to visualize.\n",
    "    Returns:\n",
    "        Plotly lines\n",
    "    \"\"\"\n",
    "#     samples = samples.view(-1,3)\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for i in range(all_samples.shape[0]):\n",
    "        samples = all_samples[i]\n",
    "        samples_init = samples[:10, :]\n",
    "        samples_mid = samples[10:50, :]\n",
    "        samples_final = samples[50:, :]\n",
    "\n",
    "        data.append(go.Scatter3d(\n",
    "        x=samples_init[:, 0],\n",
    "        y=samples_init[:, 2],\n",
    "        z=samples_init[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, color=\"blue\")\n",
    "        ))\n",
    "\n",
    "        data.append(go.Scatter3d(\n",
    "        x=samples_mid[:, 0],\n",
    "        y=samples_mid[:, 2],\n",
    "        z=samples_mid[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, color=\"black\")\n",
    "        ))\n",
    "\n",
    "        data.append(go.Scatter3d(\n",
    "        x=samples_final[:, 0],\n",
    "        y=samples_final[:, 2],\n",
    "        z=samples_final[:, 1],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, color=\"green\")\n",
    "        ))\n",
    "        \n",
    "    data.append(get_sphere(radius=1.0, color=\"#111111\", opacity=0.05))\n",
    "    data.append(get_sphere(radius=2.0, color=\"#111111\", opacity=0.05))\n",
    "    fig = go.Figure(data = data\n",
    "        \n",
    "    )\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", showspikes=False),\n",
    "            yaxis=dict(title=\"z\", showspikes=False),\n",
    "            zaxis=dict(title=\"y\", showspikes=False),\n",
    "        ),\n",
    "        margin=dict(r=0, b=10, l=0, t=10),\n",
    "        hovermode=False,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# def world2camera_viewdirs(w_viewdirs, cam2world, NS):\n",
    "#     w_viewdirs = repeat_interleave(w_viewdirs, NS)  # (SB*NS, B, 3)\n",
    "#     rot = torch.copy(cam2world[:, :3, :3]).transpose(1, 2)  # (B, 3, 3)\n",
    "#     viewdirs = torch.matmul(rot[:, None, :3, :3], w_viewdirs.unsqueeze(-1))[..., 0]\n",
    "#     return viewdirs\n",
    "\n",
    "def world2camera_viewdirs(w_viewdirs, cam2world, NS):\n",
    "    w_viewdirs = repeat_interleave(w_viewdirs, NS)  # (SB*NS, B, 3)\n",
    "    rot = cam2world[:, :3, :3].transpose(1, 2)  # (B, 3, 3)\n",
    "    viewdirs = torch.matmul(rot[:, None, :3, :3], w_viewdirs.unsqueeze(-1))[..., 0]\n",
    "    return viewdirs\n",
    "\n",
    "\n",
    "def world2camera(w_xyz, cam2world, NS=None):\n",
    "    \"\"\"Converts the points in world coordinates to camera view.\n",
    "    :param xyz: points in world coordinates (SB*NV, NC, 3)\n",
    "    :param poses: camera matrix (SB*NV, 4, 4)\n",
    "    :output points in camera coordinates (SB*NV, NC, 3)\n",
    "    : SB batch size\n",
    "    : NV number of views in each scene\n",
    "    : NC number of coordinate points\n",
    "    \"\"\"\n",
    "    #print(w_xyz.shape, cam2world.shape)\n",
    "    if NS is not None:\n",
    "        w_xyz = repeat_interleave(w_xyz, NS)  # (SB*NS, B, 3)\n",
    "    rot = cam2world[:, :3, :3].transpose(1, 2)  # (B, 3, 3)\n",
    "    trans = -torch.bmm(rot, cam2world[:, :3, 3:])  # (B, 3, 1)\n",
    "    #print(rot.shape, w_xyz.shape)\n",
    "    cam_rot = torch.matmul(rot[:, None, :3, :3], w_xyz.unsqueeze(-1))[..., 0]\n",
    "    cam_xyz = cam_rot + trans[:, None, :, 0]\n",
    "    # cam_xyz = cam_xyz.reshape(-1, 3)  # (SB*B, 3)\n",
    "    return cam_xyz\n",
    "\n",
    "def repeat_interleave(input, repeats, dim=0):\n",
    "    \"\"\"\n",
    "    Repeat interleave along axis 0\n",
    "    torch.repeat_interleave is currently very slow\n",
    "    https://github.com/pytorch/pytorch/issues/31980\n",
    "    \"\"\"\n",
    "    output = input.unsqueeze(1).expand(-1, repeats, *input.shape[1:])\n",
    "    return output.reshape(-1, *input.shape[1:])\n",
    "\n",
    "\n",
    "def projection(c_xyz, focal, c):\n",
    "    \"\"\"Converts [x,y,z] in camera coordinates to image coordinates \n",
    "        for the given focal length focal and image center c.\n",
    "    :param c_xyz: points in camera coordinates (SB*NV, NP, 3)\n",
    "    :param focal: focal length (SB, 2)\n",
    "    :c: image center (SB, 2)\n",
    "    :output uv: pixel coordinates (SB, NV, NP, 2)\n",
    "    \"\"\"\n",
    "    uv = -c_xyz[..., :2] / (c_xyz[..., 2:] + 1e-9)  # (SB*NV, NC, 2); NC: number of grid cells \n",
    "    uv *= repeat_interleave(\n",
    "                focal.unsqueeze(1), NV if focal.shape[0] > 1 else 1\n",
    "            )\n",
    "    uv += repeat_interleave(\n",
    "                c.unsqueeze(1), NV if c.shape[0] > 1 else 1\n",
    "            )\n",
    "    return uv\n",
    "\n",
    "\n",
    "def pos_enc(x, min_deg=0, max_deg=10):\n",
    "    scales = torch.tensor([2**i for i in range(min_deg, max_deg)]).type_as(x)\n",
    "    xb = torch.reshape((x[..., None, :] * scales[:, None]), list(x.shape[:-1]) + [-1])\n",
    "    four_feat = torch.sin(torch.cat([xb, xb + 0.5 * np.pi], dim=-1))\n",
    "    return torch.cat([x] + [four_feat], dim=-1)\n",
    "\n",
    "import open3d as o3d\n",
    "def get_world_grid(side_lengths, grid_size):\n",
    "    \"\"\" Returns a 3D grid of points in world coordinates.\n",
    "    :param side_lengths: (min, max) for each axis (3, 2)\n",
    "    :param grid_size: number of points along each dimension () or (3)\n",
    "    :output grid: (1, grid_size**3, 3)\n",
    "    \"\"\"\n",
    "    if len(grid_size) == 1:\n",
    "        grid_size = [grid_size[0] for _ in range(3)]\n",
    "        \n",
    "    w_x = torch.linspace(side_lengths[0][0], side_lengths[0][1], grid_size[0])\n",
    "    w_y = torch.linspace(side_lengths[1][0], side_lengths[1][1], grid_size[1])\n",
    "    w_z = torch.linspace(side_lengths[2][0], side_lengths[2][1], grid_size[2])\n",
    "    # Z, Y, X = torch.meshgrid(w_x, w_y, w_z)\n",
    "    X, Y, Z = torch.meshgrid(w_x, w_y, w_z)\n",
    "    w_xyz = torch.stack([X, Y, Z], axis=-1) # (gs, gs, gs, 3), gs = grid_size\n",
    "    print(w_xyz.shape)\n",
    "    w_xyz = w_xyz.reshape(-1, 3).unsqueeze(0) # (1, grid_size**3, 3)\n",
    "    return w_xyz\n",
    "\n",
    "def repeat_interleave(input, repeats, dim=0):\n",
    "    \"\"\"\n",
    "    Repeat interleave along axis 0\n",
    "    torch.repeat_interleave is currently very slow\n",
    "    https://github.com/pytorch/pytorch/issues/31980\n",
    "    \"\"\"\n",
    "    output = input.unsqueeze(1).expand(-1, repeats, *input.shape[1:])\n",
    "    return output.reshape(-1, *input.shape[1:])\n",
    "\n",
    "\n",
    "def intersect_sphere(rays_o, rays_d):\n",
    "    \"\"\"Compute the depth of the intersection point between this ray and unit sphere.\n",
    "    Args:\n",
    "        rays_o: [num_rays, 3]. Ray origins.\n",
    "        rays_d: [num_rays, 3]. Ray directions.\n",
    "    Returns:\n",
    "        depth: [num_rays, 1]. Depth of the intersection point.\n",
    "    \"\"\"\n",
    "    # note: d1 becomes negative if this mid point is behind camera\n",
    "\n",
    "    d1 = -torch.sum(rays_d * rays_o, dim=-1, keepdim=True) / torch.sum(\n",
    "        rays_d**2, dim=-1, keepdim=True\n",
    "    )\n",
    "    p = rays_o + d1 * rays_d\n",
    "    # consider the case where the ray does not intersect the sphere\n",
    "    rays_d_cos = 1.0 / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    p_norm_sq = torch.sum(p * p, dim=-1, keepdim=True)\n",
    "    check_pos = 1.0 - p_norm_sq\n",
    "    print(\"check pos\", torch.max(p_norm_sq), torch.min(p_norm_sq))\n",
    "    assert torch.all(check_pos >= 0), \"1.0 - p_norm_sq should be greater than 0\"\n",
    "    d2 = torch.sqrt(1.0 - p_norm_sq) * rays_d_cos\n",
    "    return d1 + d2\n",
    "\n",
    "def w2i_projection(w_xyz, cam2world, intrinsics):\n",
    "    \"\"\"Converts the points in world coordinates to camera view.\n",
    "    :param xyz: points in world coordinates (SB*NV, NC, 3)\n",
    "    :param poses: camera matrix (SB*NV, 4, 4)\n",
    "    :output points in camera coordinates (SB*NV, NC, 3)\n",
    "    : SB batch size\n",
    "    : NV number of views in each scene\n",
    "    : NC number of coordinate points\n",
    "    \"\"\"\n",
    "    w_xyz = torch.cat([w_xyz, torch.ones_like(w_xyz[..., :1])], dim=-1)  # [n_points, 4]\n",
    "    cam_xyz = torch.inverse(cam2world).bmm(w_xyz.permute(0,2,1))\n",
    "    camera_grids = cam_xyz.permute(0,2,1)[:,:,:3]\n",
    "    projections = intrinsics[None, ...].repeat(cam2world.shape[0], 1, 1).bmm(cam_xyz[:,:3,:])\n",
    "    projections = projections.permute(0,2,1)\n",
    "    \n",
    "    uv = projections[..., :2] / torch.clamp(projections[..., 2:3], min=1e-8)  # [n_views, n_points, 2]\n",
    "    uv = torch.clamp(uv, min=-1e6, max=1e6)\n",
    "    #uv = projections[..., :2] / projections[..., 2:3]  # [n_views, n_points, 2]\n",
    "    mask = projections[..., 2] > 0\n",
    "    return camera_grids, uv, mask\n",
    "\n",
    "def projection_extrinsics_alldim(w_xyz, w2c, intrinsics):\n",
    "    \"\"\"Converts the points in world coordinates to camera view.\n",
    "    :param xyz: points in world coordinates (SB*NV, NC, 3)\n",
    "    :param poses: camera matrix (SB*NV, 4, 4)\n",
    "    :output points in camera coordinates (SB*NV, NC, 3)\n",
    "    : SB batch size\n",
    "    : NV number of views in each scene\n",
    "    : NC number of coordinate points\n",
    "    \"\"\"\n",
    "    w_xyz = torch.cat([w_xyz, torch.ones_like(w_xyz[..., :1])], dim=-1)  # [n_points, 4]\n",
    "    cam_xyz = w2c.bmm(w_xyz.permute(0,2,1))\n",
    "    camera_grids = cam_xyz.permute(0,2,1)[:,:,:3]\n",
    "    projections = intrinsics[None, ...].repeat(w2c.shape[0], 1, 1).bmm(cam_xyz[:,:3,:])\n",
    "    projections = projections.permute(0,2,1)\n",
    "    return projections\n",
    "\n",
    "def projection_extrinsics(w_xyz, w2c, intrinsics):\n",
    "    \"\"\"Converts the points in world coordinates to camera view.\n",
    "    :param xyz: points in world coordinates (SB*NV, NC, 3)\n",
    "    :param poses: camera matrix (SB*NV, 4, 4)\n",
    "    :output points in camera coordinates (SB*NV, NC, 3)\n",
    "    : SB batch size\n",
    "    : NV number of views in each scene\n",
    "    : NC number of coordinate points\n",
    "    \"\"\"\n",
    "    w_xyz = torch.cat([w_xyz, torch.ones_like(w_xyz[..., :1])], dim=-1)  # [n_points, 4]\n",
    "    cam_xyz = w2c.bmm(w_xyz.permute(0,2,1))\n",
    "    camera_grids = cam_xyz.permute(0,2,1)[:,:,:3]\n",
    "    projections = intrinsics[None, ...].repeat(w2c.shape[0], 1, 1).bmm(cam_xyz[:,:3,:])\n",
    "    projections = projections.permute(0,2,1)\n",
    "    \n",
    "    uv = projections[..., :2] / torch.clamp(projections[..., 2:3], min=1e-8)  # [n_views, n_points, 2]\n",
    "    uv = torch.clamp(uv, min=-1e6, max=1e6)\n",
    "    #uv = projections[..., :2] / projections[..., 2:3]  # [n_views, n_points, 2]\n",
    "    mask = projections[..., 2] > 0\n",
    "    return camera_grids, uv, mask\n",
    "\n",
    "# def compute_projections(xyz, train_poses, train_intrinsics, NV):\n",
    "#     '''\n",
    "#     project 3D points into cameras\n",
    "#     :param xyz: [..., 3]\n",
    "#     :param train_cameras: [n_views, 34], 34 = img_size(2) + intrinsics(16) + extrinsics(16)\n",
    "#     :return: pixel locations [..., 2], mask [...]\n",
    "#     '''\n",
    "#     original_shape = xyz.shape[:2]\n",
    "#     xyz = xyz.reshape(-1, 3)\n",
    "#     num_views = NV\n",
    "#     xyz_h = torch.cat([xyz, torch.ones_like(xyz[..., :1])], dim=-1)  # [n_points, 4]\n",
    "#     print(\"train_intrinsics[None, ...].repeat(train_poses.shape[0], 1, 1)\", train_intrinsics[None, ...].repeat(train_poses.shape[0], 1, 1).shape)\n",
    "#     projections = train_intrinsics[None, ...].repeat(train_poses.shape[0], 1, 1).bmm(torch.inverse(train_poses)) \\\n",
    "#         .bmm(xyz_h.t()[None, ...].repeat(num_views, 1, 1))  # [n_views, 4, n_points]\n",
    "#     projections = projections.permute(0, 2, 1)  # [n_views, n_points, 4]\n",
    "#     pixel_locations = projections[..., :2] / torch.clamp(projections[..., 2:3], min=1e-8)  # [n_views, n_points, 2]\n",
    "#     pixel_locations = torch.clamp(pixel_locations, min=-1e6, max=1e6)\n",
    "#     mask = projections[..., 2] > 0   # a point is invalid if behind the camera\n",
    "#     return pixel_locations.reshape((num_views, ) + original_shape + (2, )), \\\n",
    "#             mask.reshape((num_views, ) + original_shape)\n",
    "\n",
    "# def normalize(pixel_locations, h, w):\n",
    "#     resize_factor = torch.tensor([w-1., h-1.]).to(pixel_locations.device)[None, None, :]\n",
    "#     normalized_pixel_locations = 2 * pixel_locations / resize_factor - 1.  # [n_views, n_points, 2]\n",
    "#     return normalized_pixel_locations\n",
    "    \n",
    "# def compute(xyz, poses, imgs, K, NV, H, W):\n",
    "\n",
    "#     pixel_locations, mask_in_front = compute_projections(xyz, poses, K, NV)\n",
    "#     normalized_pixel_locations = normalize(pixel_locations, H, W)   # [n_views, n_rays, n_samples, 2]\n",
    "\n",
    "#     # rgb sampling\n",
    "#     rgbs_sampled = F.grid_sample(imgs, normalized_pixel_locations, align_corners=True)\n",
    "#     rgb_sampled = rgbs_sampled.permute(2, 3, 0, 1)  # [n_rays, n_samples, n_views, 3]\n",
    "#     inbound = inbound(pixel_locations, H, W)\n",
    "\n",
    "#     return rgb_sampled, inbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82b29dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rays_o' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20567/43081343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mt_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrays_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mt_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt_rand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rays_o' is not defined"
     ]
    }
   ],
   "source": [
    "far = 3.0\n",
    "t_vals = torch.linspace(0.0, 1.0, 10 + 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mids = 0.5 * (t_vals[..., 1:] + t_vals[..., :-1])\n",
    "upper = torch.cat([mids, t_vals[..., -1:]], -1)\n",
    "lower = torch.cat([t_vals[..., :1], mids], -1)\n",
    "t_rand = torch.rand((1, 10 + 1), device=rays_o.device)\n",
    "t_vals = lower + (upper - lower) * t_rand\n",
    "\n",
    "t_vals_linear = 1.0 * (1.0 - t_vals) + 3.0 * t_vals\n",
    "\n",
    "print(\"t_vals\", t_vals)\n",
    "print(\"t_vals_linear\", t_vals_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "src_views_num = np.random.choice(100, 3, replace=False)\n",
    "dest_view_num = random.choice(src_views_num)\n",
    "\n",
    "print(\"src_views_num, dest_view_num\", src_views_num, dest_view_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2202d69",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47116a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train dataset 200\n",
      "rays torch.Size([1, 4096, 3])\n",
      "rgbs torch.Size([1, 4096, 3])\n",
      "instance_mask torch.Size([1, 4096])\n",
      "instance_ids torch.Size([1, 4096])\n",
      "===============================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = dataset_dict['pd_multi_obj_ae_nocs']\n",
    "# dataset = dataset_dict['pd_multi_obj']\n",
    "dataset = dataset_dict['pd_multi']\n",
    "# root_dir = '/home/zubairirshad/pd-api-py/PD_v3_eval/test_novelobj'\n",
    "# root_dir = '/home/zubairirshad/pd-api-py/PD_v3_eval/test_novelobj/SF_6thAndMission_medium0'\n",
    "root_dir = '/home/zubairirshad/pd-api-py/PDStepv3/train'\n",
    "img_wh = (320, 240)\n",
    "\n",
    "kwargs = {'root_dir': root_dir,\n",
    "          'img_wh': tuple(img_wh),\n",
    "         'split': 'train',\n",
    "#          'eval_inference':'sapien',\n",
    "         'white_back': True}\n",
    "\n",
    "# kwargs = {'root_dir': root_dir,\n",
    "#           'img_wh': tuple(img_wh),\n",
    "#          'model_type': 'nerfpp',\n",
    "#          'split': 'test',\n",
    "#          'eval_inference': '3view_testnovelobj'}\n",
    "\n",
    "train_dataset = dataset(**kwargs)\n",
    "dataloader =  DataLoader(train_dataset,\n",
    "                  shuffle=False,\n",
    "                  num_workers=0,\n",
    "                  batch_size=1,\n",
    "                  pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"len train dataset\", len(train_dataset))\n",
    "for i, data in enumerate(dataloader):\n",
    "    if i>0:\n",
    "        break\n",
    "    for k,v in data.items():\n",
    "        if k =='deg':\n",
    "            continue\n",
    "        print(k,v.shape)\n",
    "\n",
    "\n",
    "    print(\"===============================\\n\\n\\n\")\n",
    "    \n",
    "# for k,v in data.items():\n",
    "#     if k =='deg':\n",
    "#         continue\n",
    "#     data[k] = v.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783d73d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[instance ids] torch.Size([1, 4096])\n",
      "len train_dataset ids 150\n",
      "data[instance_ids] tensor([[107, 107, 107,  ..., 107, 107, 107]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[240, 320, 3]' is invalid for input of size 12288",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20567/1550222141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len train_dataset ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data[instance_ids]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"instance_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rgbs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# for i, data in enumerate(dataloader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[240, 320, 3]' is invalid for input of size 12288"
     ]
    }
   ],
   "source": [
    "# w,h = data[\"img_wh\"]\n",
    "# print(\"deg\", np.rad2deg(data['deg']))\n",
    "# plt.imshow(data[\"src_imgs\"].permute(1,2,0).numpy())\n",
    "# plt.show()\n",
    "print(\"data[instance ids]\", data[\"instance_ids\"].shape)\n",
    "print(\"len train_dataset ids\", len(train_dataset.ids))\n",
    "print(\"data[instance_ids]\", data[\"instance_ids\"])\n",
    "plt.imshow(data[\"rgbs\"].reshape(240,320,3).numpy())\n",
    "plt.show()\n",
    "# for i, data in enumerate(dataloader):\n",
    "#     print(\"i\", i)\n",
    "#     plt.imshow(data[\"target\"].squeeze(0).reshape(240,320, 3).numpy())\n",
    "#     plt.show()\n",
    "# plt.show()\n",
    "# a = (data[\"instance_mask\"]>0).sum()\n",
    "# print(\"data[instance mask]\", a)\n",
    "\n",
    "# print(\"near obj\", near_obj[0].shape)\n",
    "\n",
    "# image = data[\"rgbs\"].reshape(240,320,3).nuprint(\"len train dataset\", len(train_dataset))\n",
    "# for i, data in enumerate(dataloader):\n",
    "#     print(\"i\", i)\n",
    "#     for k,v in data.items():\n",
    "#         print(k,v.squeeze(0).shape)\n",
    "#     if i>0:\n",
    "#         break\n",
    "\n",
    "#     print(\"===============================\\n\\n\\n\")mpy()\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "# print(\"data[rays]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def unprocess_images(normalized_images, shape = ()):\n",
    "    inverse_transform = T.Compose([T.Normalize((-0.5/0.5, -0.5/0.5, -0.5/0.5), (1/0.5, 1/0.5, 1/0.5))])\n",
    "    return inverse_transform(normalized_images)\n",
    "NV=3\n",
    "\n",
    "def convert_pose(C2W):\n",
    "    flip_yz = torch.eye(4)\n",
    "    flip_yz[1, 1] = -1\n",
    "    flip_yz[2, 2] = -1\n",
    "    C2W = torch.matmul(C2W, flip_yz)\n",
    "    return C2W\n",
    "\n",
    "for k,v in data.items():\n",
    "    data[k] = v.squeeze(0)\n",
    "    \n",
    "new_src_imgs = unprocess_images(data[\"src_imgs\"])\n",
    "for i in range(NV):\n",
    "    if i ==0 or i==1:\n",
    "        plt.imshow(new_src_imgs[i].permute(1,2,0).numpy())\n",
    "        plt.show()\n",
    "    \n",
    "img_1 = new_src_imgs[0].permute(1,2,0).numpy()\n",
    "img_2 = new_src_imgs[1].permute(1,2,0).numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pose_1 = np.linalg.inv(convert_pose(data[\"src_poses\"][0]).numpy())\n",
    "# pose_2 = np.linalg.inv(convert_pose(data[\"src_poses\"][1]).numpy())\n",
    "\n",
    "pose_1 = data[\"w2cs\"][0].numpy()\n",
    "pose_2 = data[\"w2cs\"][1].numpy()\n",
    "\n",
    "from models.nerfplusplus.util import verify_data\n",
    "\n",
    "\n",
    "K = torch.FloatTensor([\n",
    "    [data[\"src_focal\"][0], 0., data[\"src_c\"][0][0]],\n",
    "    [0., data[\"src_focal\"][0], data[\"src_c\"][0][1]],\n",
    "    [0., 0., 1.],\n",
    "])\n",
    "im = verify_data(np.uint8(img_1*255.), np.uint8(img_2*255.),\n",
    "                 K, pose_1,\n",
    "                 K, pose_2)\n",
    "im = im/255\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f4de8",
   "metadata": {},
   "source": [
    "### Get Samples and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcbe16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import models.nerfplusplus.helper as helper\n",
    "\n",
    "\n",
    "# for k,v in data.items():\n",
    "#     data[k] = v.squeeze(0)\n",
    "    \n",
    "nerfplusplus = True\n",
    "contract = False\n",
    "\n",
    "if nerfplusplus:\n",
    "    import models.nerfplusplus.helper as helper\n",
    "    from torch import linalg as LA\n",
    "    near = torch.full_like(data[\"rays_o\"][..., -1:], 1e-4)\n",
    "    far = intersect_sphere(data[\"rays_o\"], data[\"rays_d\"])\n",
    "    \n",
    "else:\n",
    "    import models.vanilla_nerf.helper as helper\n",
    "    from torch import linalg as LA\n",
    "    near = 0.2\n",
    "    far = 3.0\n",
    "\n",
    "if nerfplusplus:\n",
    "    obj_t_vals, obj_samples = sample_along_rays(\n",
    "        rays_o=data[\"rays_o\"],\n",
    "        rays_d=data[\"rays_d\"],\n",
    "        num_samples=65,\n",
    "        near = near,\n",
    "        far = far,\n",
    "        randomized=True,\n",
    "        lindisp=False,\n",
    "        in_sphere=True,\n",
    "    )\n",
    "\n",
    "    bg_t_vals, bg_samples, bg_samples_linear = sample_along_rays(\n",
    "        rays_o=data[\"rays_o\"],\n",
    "        rays_d=data[\"rays_d\"],\n",
    "        num_samples=65,\n",
    "        near=near,\n",
    "        far=far,\n",
    "        randomized=True,\n",
    "        lindisp=False,\n",
    "        in_sphere=False,\n",
    "    )\n",
    "    print(\"toch max min bg_samples linear\", torch.max(bg_samples_linear), torch.min(bg_samples_linear))\n",
    "else:\n",
    "    all_t_vals, all_samples = helper.sample_along_rays(\n",
    "        rays_o=data[\"rays_o\"],\n",
    "        rays_d=data[\"rays_d\"],\n",
    "        num_samples=65,\n",
    "        near=near,\n",
    "        far=far,\n",
    "        randomized=True,\n",
    "        lindisp=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "def reverse_contract_pts(pts, radius):\n",
    "    norm_pts = torch.norm(pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1 + 0.2) - 0.2 / norm_pts) * (pts / norm_pts) * radius\n",
    "    return contracted_points\n",
    "\n",
    "def contract_pts(pts, radius=3):\n",
    "    mask = torch.norm(pts, dim=-1).unsqueeze(-1) > radius\n",
    "    new_pts = pts.clone()/radius\n",
    "    norm_pts = torch.norm(new_pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1+0.5) - 0.5/(norm_pts))*(new_pts/norm_pts)*radius\n",
    "    warped_points = torch.where(\n",
    "        mask == False, pts, mask*contracted_points \n",
    "    )\n",
    "    \n",
    "    return warped_points\n",
    "\n",
    "def uncontract_pts(pts, radius=1):\n",
    "    mask = torch.norm(pts, dim=-1).unsqueeze(-1) > radius\n",
    "    new_pts = pts.clone()/radius\n",
    "    norm_pts = torch.norm(new_pts, dim=-1).unsqueeze(-1)\n",
    "    contracted_points = ((1+0.6) - 0.6/(norm_pts))*(new_pts/norm_pts)*radius\n",
    "    warped_points = torch.where(\n",
    "        mask == False, pts, mask*contracted_points \n",
    "    )\n",
    "    \n",
    "    return warped_points\n",
    "\n",
    "def contract_samples(x, order=2):\n",
    "    mag = LA.norm(x, order, dim=-1)[..., None]\n",
    "    return torch.where(mag < 1, x, (2 - (1 / mag)) * (x / mag))\n",
    "\n",
    "# def inverse_contract_samples(x, order = order):\n",
    "#     mag = torch.linalg.norm(x, ord=order, dim=-1)[..., None]\n",
    "#     mask = mag < 1\n",
    "#     expanded = x * mag*mag / (2 - (1 / mag))\n",
    "#     return torch.where(mask, x, expanded)\n",
    "\n",
    "def _contract(x):\n",
    "    x_mag_sq = torch.sum(x**2, dim=-1, keepdim=True).clip(min=1e-32)\n",
    "    z = torch.where(\n",
    "        x_mag_sq <= 1, x, ((2 * torch.sqrt(x_mag_sq) - 1) / x_mag_sq) * x\n",
    "    )\n",
    "    return z\n",
    "\n",
    "def _inverse_contract(x):\n",
    "    x_mag_sq = torch.sum(x**2, dim=-1, keepdim=True).clip(min=1e-32)\n",
    "    z = torch.where(\n",
    "        x_mag_sq <= 1, x, x * (x_mag_sq / (2 * torch.sqrt(x_mag_sq) - 1))\n",
    "    )\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "if nerfplusplus:\n",
    "#     all_samples = torch.cat((obj_samples, bg_samples[:,:,:3]), dim=0)\n",
    "#     all_samples = torch.cat((obj_samples, bg_samples_linear), dim=0)\n",
    "\n",
    "    all_samples = obj_samples\n",
    "    print(\"all_samples\", all_samples.shape)\n",
    "else:\n",
    "    if contract:\n",
    "        all_samples = contract_samples(all_samples, float('inf'))\n",
    "        print(\"torch min max  all samples\", torch.min(samples), torch.max(samples))\n",
    "#         all_samples = _inverse_contract(all_samples)\n",
    "\n",
    "\n",
    "if nerfplusplus:\n",
    "    print(\"bg obj samples\",bg_samples.shape, obj_samples.shape)\n",
    "else:\n",
    "    print(\"all_samples\",all_samples.shape)\n",
    "\n",
    "\n",
    "coords = get_image_coords(pixel_offset = 0.5,image_height = 120, image_width = 160)\n",
    "print(coords.shape)\n",
    "\n",
    "rays_o = data[\"rays_o\"][:20,:]\n",
    "rays_d = data[\"rays_d\"][:20,:]\n",
    "coords = coords[:20,:]\n",
    "\n",
    "if nerfplusplus:\n",
    "    num = np.random.choice(all_samples.shape[0], 5)\n",
    "    samples = all_samples[num,:, :3]\n",
    "    \n",
    "#     num_bg = np.random.choice(bg_samples.shape[0], 20)\n",
    "    samples_bg = bg_samples[num,:, :3]\n",
    "    \n",
    "    print(\"inverse radius \", bg_samples[:,:,3])\n",
    "    \n",
    "#     num_bg_linear = np.random.choice(bg_samples.shape[0], 20)\n",
    "    samples_bg_linear = bg_samples_linear[num,:, :3]\n",
    "#     print(samples.shape, samples_bg.shape)\n",
    "    samples = torch.cat((samples, samples_bg, samples_bg_linear), dim=0)\n",
    "#     samples = samples_bg_linear\n",
    "else:\n",
    "    num = np.random.choice(all_samples.shape[0], 20)\n",
    "    samples = all_samples[num,:, :3]\n",
    "\n",
    "\n",
    "fig = vis_camera_samples(samples)\n",
    "fig.show()\n",
    "\n",
    "print(\"torch min max samples\", torch.min(samples), torch.max(samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4dd0fb",
   "metadata": {},
   "source": [
    "### IBRNet feature sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xyz = all_samples[:,:,:3]\n",
    "w_xyz = w_xyz.reshape(-1,3).unsqueeze(0)\n",
    "poses = torch.clone(data[\"src_poses\"])\n",
    "all_c2w = [convert_pose(pose).unsqueeze(0) for pose in poses]\n",
    "poses = torch.cat(all_c2w, dim=0).to(dtype = torch.float)\n",
    "\n",
    "new_images = unprocess_images(data[\"src_imgs\"])\n",
    "\n",
    "NV = 3\n",
    "\n",
    "w_xyz = repeat_interleave(w_xyz,NV) # (SB*NV, NC, 3) NC: number of grid cells\n",
    "\n",
    "K = torch.FloatTensor([\n",
    "    [data[\"src_focal\"][0], 0., data[\"src_c\"][0][0]],\n",
    "    [0., data[\"src_focal\"][0], data[\"src_c\"][0][1]],\n",
    "    [0., 0., 1.],\n",
    "])\n",
    "\n",
    "height, width = new_images.size()[2:]\n",
    "print(\"poses\", poses.shape, w_xyz.shape)\n",
    "\n",
    "proj_mat = data[\"proj_mats\"][0].unsqueeze(0)\n",
    "\n",
    "w_xyz = all_samples[:,:,:3].reshape(-1,3).unsqueeze(0)\n",
    "print(\"w_xyz\", w_xyz.shape)\n",
    "\n",
    "world_xyz = repeat_interleave(w_xyz, NV)  # (SB*NS, B, 3)\n",
    "extrinsics = data[\"w2cs\"]\n",
    "print(\"extrinsics\", extrinsics.shape, world_xyz.shape)\n",
    "intrinsics = K\n",
    "uv =  projection_extrinsics_alldim(world_xyz, extrinsics, intrinsics)\n",
    "\n",
    "# projections  = torch.bmm(proj_mat, w_xyz.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "# print(\"projections\", projections.shape)\n",
    "# uv = projections[..., :2] / torch.clamp(projections[..., 2:3], min=1e-8)  # [n_views, n_points, 2]\n",
    "# uv = torch.clamp(uv, min=-1e6, max=1e6)\n",
    "# print(\"uv\",uv.shape)\n",
    "\n",
    "im_x = uv[:,:, 0]\n",
    "im_y = uv[:,:, 1]\n",
    "pixel_locations = torch.stack([im_x,im_y], dim=-1)\n",
    "im_grid = torch.stack([2 * im_x / (width - 1) - 1, 2 * im_y / (height - 1) - 1], dim=-1)\n",
    "print(\"im_grid\", im_grid.shape)\n",
    "\n",
    "\n",
    "    \n",
    "imgs = unprocess_images(data[\"src_imgs\"])\n",
    "imgs = F.interpolate(imgs, size=(120,160), mode='bilinear', align_corners=False)\n",
    "\n",
    "K = K/2\n",
    "K[-1,1] = 1\n",
    "plt.imshow(imgs[0].permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "\n",
    "grid = im_grid\n",
    "grid = grid.unsqueeze(2)\n",
    "\n",
    "data_im = F.grid_sample(imgs, grid, align_corners=True, mode='bilinear', padding_mode='zeros')\n",
    "print(\"data_im\", data_im.shape)\n",
    "all_imgs = data_im.squeeze(-1).permute(0, 2,1).reshape(NV, 240,320,66,3).numpy()\n",
    "img_1 = all_imgs[2, :,:,65,:]\n",
    "plt.imshow(img_1)\n",
    "plt.show()\n",
    "print(\"data_im\", data_im.shape)\n",
    "\n",
    "mask = (im_grid.abs() <= 1).float()\n",
    "mask = (mask[...,0]*mask[...,1]).float()\n",
    "print(mask.shape)\n",
    "\n",
    "\n",
    "\n",
    "feats_c = data_im.squeeze(-1).permute(2,0,1)\n",
    "print(\"feats_c\", feats_c.shape)\n",
    "\n",
    "# .view(-1,9)\n",
    "\n",
    "\n",
    "print(\"feat_c\", feats_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63af795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_grid(samples, volume_features, w2cs, focal, c, near=0.2, far = 2.5):\n",
    "    \"\"\"\n",
    "    Get pixel-aligned image features at 2D image coordinates\n",
    "    :param uv (B, N, 2) image points (x,y)\n",
    "    :param cam_z ignored (for compatibility)\n",
    "    :param image_size image size, either (width, height) or single int.\n",
    "    if not specified, assumes coords are in [-1, 1]\n",
    "    :param z_bounds ignored (for compatibility)\n",
    "    :return (B, L, N) L is latent size\n",
    "    \"\"\" \n",
    "\n",
    "    w2c_ref = w2cs[0].unsqueeze(0)\n",
    "    _,_,_, H, W = volume_features.shape\n",
    "    inv_scale = torch.tensor([W-1, H-1]).to(w2c_ref.device)\n",
    "\n",
    "    samples = samples.reshape(-1,3).unsqueeze(0)\n",
    "    intrinsics_ref = torch.FloatTensor([\n",
    "        [focal[0], 0., c[0][0]],\n",
    "        [0., focal[0], c[0][1]],\n",
    "        [0., 0., 1.],\n",
    "        ]).to(w2c_ref.device)\n",
    "    intrinsics_ref = intrinsics_ref/2\n",
    "    intrinsics_ref[-1,-1] = 1\n",
    "\n",
    "    if intrinsics_ref is not None:\n",
    "        point_samples_pixel = projection_extrinsics_alldim(samples, w2c_ref, intrinsics_ref)\n",
    "        point_samples_pixel = point_samples_pixel.squeeze(0)\n",
    "        point_samples_pixel[:,:2] = (point_samples_pixel[:,:2] / point_samples_pixel[:,-1:] + 0.0) / inv_scale.reshape(1,2)  \n",
    "        point_samples_pixel[:,2] = (point_samples_pixel[:,2] - near) / (far - near)  # normalize to 0~1\n",
    "    \n",
    "    grid = point_samples_pixel.view(1, 1, 1, -1, 3) * 2 - 1.0  # [1 1 H W 3] (x,y,z)\n",
    "    features = F.grid_sample(volume_features, grid, align_corners=True, mode='bilinear')[:,:,0].permute(2,3,0,1).squeeze()#, padding_mode=\"border\"\n",
    "\n",
    "    return features\n",
    "\n",
    "samples = torch.randn((2048,65,3))\n",
    "volume_features = torch.randn((1, 8, 128, 120, 160))\n",
    "imgs = torch.randn((3, 3, 240, 320))\n",
    "w2cs = torch.randn((3,4,4))\n",
    "focal = torch.randn((3))\n",
    "c = torch.randn((3,2))\n",
    "print(\"samples\", samples.shape, volume_features.shape)\n",
    "features = index_grid(samples, volume_features, w2cs, focal, c, 240, 320)\n",
    "\n",
    "print(\"features\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e520c5",
   "metadata": {},
   "source": [
    "\n",
    "### Encode for PN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d379c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from models.vanilla_nerf.encoder import *\n",
    "NV = 3\n",
    "pn = True\n",
    "def unprocess_images(normalized_images, shape = ()):\n",
    "    inverse_transform = T.Compose([T.Normalize((-0.5/0.5, -0.5/0.5, -0.5/0.5), (1/0.5, 1/0.5, 1/0.5))])\n",
    "    print(\"unnormalize dimgs\", inverse_transform(normalized_images).shape)\n",
    "    return inverse_transform(normalized_images)\n",
    "\n",
    "w_xyz = all_samples[:,:,:3]\n",
    "print(\"w_xyz\", w_xyz.shape)\n",
    "\n",
    "B, N_samples, _ = w_xyz.shape\n",
    "w_xyz = w_xyz.reshape(-1,3).unsqueeze(0)\n",
    "\n",
    "# print(\"wxyz, poses\", w_xyz.shape, poses.shape)\n",
    "poses = torch.clone(data[\"src_poses\"])\n",
    "cam_xyz = world2camera(w_xyz, poses, NV)\n",
    "\n",
    "print(\"cam_xyz\", cam_xyz.shape)\n",
    "\n",
    "def inbound(pixel_locations, h, w):\n",
    "    '''\n",
    "    check if the pixel locations are in valid range\n",
    "    :param pixel_locations: [..., 2]\n",
    "    :param h: height\n",
    "    :param w: weight\n",
    "    :return: mask, bool, [...]\n",
    "    '''\n",
    "    return (pixel_locations[..., 0] <= w - 1.) & \\\n",
    "           (pixel_locations[..., 0] >= 0) & \\\n",
    "           (pixel_locations[..., 1] <= h - 1.) &\\\n",
    "           (pixel_locations[..., 1] >= 0)\n",
    "\n",
    "\n",
    "\n",
    "encoder = SpatialEncoder(backbone=\"resnet34\",\n",
    "                                              pretrained=True,\n",
    "                                              num_layers=4,\n",
    "                                              index_interp=\"bilinear\",\n",
    "                                              index_padding=\"zeros\",\n",
    "                                              # index_padding=\"border\",\n",
    "                                              upsample_interp=\"bilinear\",\n",
    "                                              feature_scale=1.0,\n",
    "                                              use_first_pool=True,\n",
    "                                              norm_type=\"batch\")\n",
    "\n",
    "latent = encoder(data[\"src_imgs\"])\n",
    "print(latent.shape)\n",
    "# height, width = latent.size()[2:]\n",
    "\n",
    "\n",
    "\n",
    "# after encoder unnormalize images\n",
    "\n",
    "for i in range(NV):\n",
    "    plt.imshow(data[\"src_imgs\"][i].permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "    \n",
    "new_images = unprocess_images(data[\"src_imgs\"])\n",
    "\n",
    "# new_images = F.interpolate(new_images, size=(120,160), mode='bilinear', align_corners=False)\n",
    "\n",
    "for i in range(NV):\n",
    "    plt.imshow(new_images[i].permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "\n",
    "print(\"new_images\", new_images.shape)\n",
    "height, width = new_images.size()[2:]\n",
    "\n",
    "print(\"height, width\", height, width)\n",
    "\n",
    "if pn:\n",
    "    #PN projection\n",
    "    focal = data[\"src_focal\"][0].unsqueeze(-1).repeat((1, 2))\n",
    "    focal[..., 1] *= -1.0\n",
    "    c = data[\"src_c\"][0].unsqueeze(0)\n",
    "    uv_pn = projection(cam_xyz, focal, c)\n",
    "\n",
    "    im_x = uv_pn[:,:, 0]\n",
    "    im_y = uv_pn[:,:, 1]\n",
    "    pixel_locations = torch.stack([im_x,im_y], dim=-1)\n",
    "#     mask_inbound = inbound(pixel_locations, height, width)\n",
    "\n",
    "    im_grid = torch.stack([2 * im_x / (width - 1) - 1, 2 * im_y / (height - 1) - 1], dim=-1)\n",
    "\n",
    "    mask_z = cam_xyz[:,:,2]<1e-3\n",
    "\n",
    "    mask = im_grid.abs() <= 1\n",
    "    print(\"MASKKKK, MASKKKK_ZZZZZZZ\", mask.shape, mask_z.shape)\n",
    "    in_mask = (mask[...,0]*mask[...,1]).float()\n",
    "    print(\"in_mask!!!!!!!!!!!!!!!!!!\", in_mask.shape)\n",
    "    print(\"mask\", mask.shape)\n",
    "    mask = (mask.sum(dim=-1) == 2) & (mask_z)\n",
    "    #mask = mask_inbound & mask_z\n",
    "    print(\"mask\", mask.shape)\n",
    "    \n",
    "\n",
    "    print(\"mask\", mask.shape)\n",
    "    \n",
    "    print(\"torch min max im_grid_pn\", torch.min(im_grid), torch.max(im_grid))\n",
    "    min_deg_point = 0\n",
    "    max_deg_point = 10\n",
    "    samples_cam = cam_xyz\n",
    "    samples_enc = helper.pos_enc(\n",
    "        samples_cam,\n",
    "        min_deg_point,\n",
    "        max_deg_point,\n",
    "    )\n",
    "\n",
    "    viewdirs = world2camera_viewdirs(data[\"viewdirs\"].unsqueeze(0), poses, NV)\n",
    "    viewdirs_enc = helper.pos_enc(viewdirs, 0, 4)\n",
    "    a = torch.tile(viewdirs_enc[:, None, :], (1, N_samples, 1))\n",
    "    viewdirs_enc = torch.tile(viewdirs_enc[:, None, :], (1, N_samples, 1)).reshape(\n",
    "            NV, -1, viewdirs_enc.shape[-1]\n",
    "        )\n",
    "    print(\"viewdirs_enc\", viewdirs_enc.shape)\n",
    "    \n",
    "else:\n",
    "    focal = data[\"src_focal\"]\n",
    "    c = data[\"src_c\"]\n",
    "\n",
    "    print(\"data src focl, src c\",\"data[src_focal]\", data[\"src_focal\"], data[\"src_c\"])\n",
    "    K = torch.FloatTensor([\n",
    "        [data[\"src_focal\"][0], 0., data[\"src_c\"][0][0]],\n",
    "        [0., data[\"src_focal\"][0], data[\"src_c\"][0][1]],\n",
    "        [0., 0., 1.],\n",
    "    ])\n",
    "    poses = data[\"src_poses\"]\n",
    "    all_c2w = [convert_pose(pose).unsqueeze(0) for pose in poses]\n",
    "    poses = torch.cat(all_c2w, dim=0).to(dtype = torch.float)\n",
    "\n",
    "    world_xyz = repeat_interleave(w_xyz, NV)  # (SB*NS, B, 3)\n",
    "    cam_xyz, uv_rt, mask= w2i_projection(world_xyz, poses, K)\n",
    "    im_x = uv_rt[:,:, 0]\n",
    "    im_y = uv_rt[:,:, 1]\n",
    "    pixel_locations = torch.stack([im_x,im_y], dim=-1)\n",
    "    mask_inbound = inbound(pixel_locations, height, width)\n",
    "    \n",
    "    print(\"mask_inbound\", mask_inbound.shape)\n",
    "    print(\"mask_inbound\", ((mask_inbound[0]>0)==True).sum())\n",
    "    \n",
    "    im_grid = torch.stack([2 * im_x / (width - 1) - 1, 2 * im_y / (height - 1) - 1], dim=-1)\n",
    "    mask_z = cam_xyz[:,:,2]>0\n",
    "    mask = im_grid.abs() <= 1\n",
    "    \n",
    "    print(\"mask\", mask.shape)\n",
    "    mask = (mask.sum(dim=-1) == 2) & (mask_z) & mask_inbound\n",
    "\n",
    "    \n",
    "    print(\"torch min max im_grid\", torch.min(im_grid), torch.max(im_grid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((3, 262144))\n",
    "\n",
    "b = torch.randn((3, 262144, 2))\n",
    "\n",
    "c = a&b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec178e5d",
   "metadata": {},
   "source": [
    "### Plot sampled projected pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb9de4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"data[src_imgs]\", torch.min(new_images[0]), torch.max(new_images[0]))\n",
    "print(\"data[src_imgs]\", new_images.shape)\n",
    "# for i in range(NV):\n",
    "#     plt.imshow(new_images[i].permute(1,2,0).numpy())\n",
    "#     plt.show()\n",
    "\n",
    "imgs = new_images\n",
    "grid = im_grid\n",
    "grid = grid.unsqueeze(2)\n",
    "\n",
    "\n",
    "\n",
    "# mask = grid.abs() <= 1\n",
    "# za_inbound_mask = (mask ==True).sum()\n",
    "# print(\"za_inbound_mask\", za_inbound_mask, (za_inbound_mask/(grid.shape[0]*grid.shape[1]*grid.shape[-1]))*100)\n",
    "#sampled_images = F.grid_sample(imgs, grid, align_corners=True, mode='bilinear', padding_mode=\"zeros\")\n",
    "\n",
    "V = NV\n",
    "C=3\n",
    "# colors = torch.empty((grid.shape[1], V*C), device=imgs.device, dtype=torch.float)\n",
    "# print(\"colors\", colors.shape)\n",
    "colors = []\n",
    "for i, idx in enumerate(range(imgs.shape[0])):\n",
    "    print(\"imgs[idx, :, :, :].unsqueeze(0)\", imgs[idx, :, :, :].unsqueeze(0).shape)\n",
    "    print(\"grid[idx, :, :].unsqueeze(0)\", grid[idx, :, :].unsqueeze(0).shape)\n",
    "    \n",
    "    print(\"grid\", grid.shape, imgs.shape)\n",
    "    data_im = F.grid_sample(imgs[idx, :, :, :].unsqueeze(0), grid[idx, :, :].unsqueeze(0), align_corners=True, mode='bilinear', padding_mode='zeros')\n",
    "    print(\"mask, data_im\", mask.shape, data_im.shape)\n",
    "#     data_im[mask.unsqueeze(0).unsqueeze(-1)==False] = 0\n",
    "\n",
    "    # Vis\n",
    "    print(\"data_im[0].permute(1, 2, 0)\", data_im[0].squeeze(-1).permute(1,0).shape)\n",
    "    colors.append(data_im.squeeze(-1).permute(0,1,2))\n",
    "    #colors[...,i*C:i*C+C] = data_im[0].squeeze(-1).permute(1,0)\n",
    "    all_imgs = data_im.squeeze(-1).squeeze(0).permute(1,0).reshape(240,320,66,3).numpy()\n",
    "    img_1 = all_imgs[:,:,65,:]\n",
    "    plt.imshow(img_1)\n",
    "    plt.show()\n",
    "    \n",
    "colors = torch.cat(colors, dim=0)\n",
    "\n",
    "colors = colors.permute(0,2,1)\n",
    "print(\"colors\", colors.shape)\n",
    "\n",
    "colors[mask.unsqueeze(-1).repeat(1,1,colors.shape[-1])==False] = 0\n",
    "\n",
    "for i in range(colors.shape[0]):\n",
    "    \n",
    "    all_imgs = colors[i].reshape(240,320,66,3).numpy()\n",
    "    img_1 = all_imgs[:,:,65,:]\n",
    "    plt.imshow(img_1)\n",
    "    plt.show()\n",
    "\n",
    "print(colors.shape)\n",
    "\n",
    "# print(\"samples image >0\", ((sampled_images>0) ==True).sum())\n",
    "# print(\"sampled_images\", sampled_images.shape)\n",
    "\n",
    "# for i in range(sampled_images.shape[0]):\n",
    "#     if i!=1:\n",
    "#         continue\n",
    "#     all_imgs = sampled_images[i].squeeze(-1).permute(1,0).reshape(240,320,65, 3).numpy()\n",
    "#     img_1 = all_imgs[:,:,50,:]\n",
    "#     plt.imshow(img_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afcbd2b",
   "metadata": {},
   "source": [
    "### Get world grid and define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_length = 1.0\n",
    "grid_size=[256, 256, 256]\n",
    "sfactor=8\n",
    "\n",
    "world_grid = get_world_grid([[-side_length, side_length],\n",
    "                                       [-side_length, side_length],\n",
    "                                       [0, side_length],\n",
    "                                       ], [int(grid_size[0]/sfactor), int(grid_size[1]/sfactor), int(grid_size[2]/sfactor)] )  # (1, grid_size**3, 3)\n",
    "\n",
    "print(world_grid.shape)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(world_grid.squeeze(0).numpy())\n",
    "\n",
    "o3d.visualization.draw_plotly([pcd])\n",
    "\n",
    "world_grids = repeat_interleave(world_grid.clone(),\n",
    "                                          1 * 1 * 3) # (SB*NV, NC, 3) NC: number of grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(world_grids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69f843",
   "metadata": {},
   "source": [
    "### Get uv projection and K*[R\\T] projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = latent.size()[2:]\n",
    "focal = data[\"src_focal\"]/2\n",
    "# focal[..., 1] *= -1.0\n",
    "c = data[\"src_c\"]/2\n",
    "K = torch.FloatTensor([\n",
    "    [data[\"src_focal\"][0], 0., data[\"src_c\"][0][0]],\n",
    "    [0., data[\"src_focal\"][1], data[\"src_c\"][0][1]],\n",
    "    [0., 0., 1.],\n",
    "])\n",
    "\n",
    "poses = data[\"src_poses\"]\n",
    "\n",
    "\n",
    "cam_xyz, uv = w2i_projection(world_grids, poses, K)\n",
    "image_points = uv[1,:,:]\n",
    "height, width = latent.size()[2:]\n",
    "print(height, width)\n",
    "inbound = torch.logical_and(np.logical_and(image_points[:, 0] > 0, image_points[:, 0] < width),\n",
    "                      np.logical_and(image_points[:, 1] > 0, image_points[:, 1] < height))\n",
    "print(\"inbound uv\",(inbound ==True).sum()) \n",
    "\n",
    "\n",
    "camera_grids_w2c = world2camera(world_grids, poses)\n",
    "focal_uv = data[\"src_focal\"][0].unsqueeze(-1).repeat((1, 2))\n",
    "focal_uv[..., 1] *= -1.0\n",
    "c_uv = data[\"src_c\"][0].unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_points = uv_projection[1,:,:]\n",
    "inbound = torch.logical_and(np.logical_and(image_points[:, 0] > 0, image_points[:, 0] < width),\n",
    "                      np.logical_and(image_points[:, 1] > 0, image_points[:, 1] < height))\n",
    "\n",
    "print(\"inbound projection\",(inbound ==True).sum()) \n",
    "\n",
    "diff = torch.norm(camera_grids_w2c- cam_xyz)\n",
    "print(\"diff\",diff)\n",
    "\n",
    "diff_uv = torch.norm(uv_projection- uv)\n",
    "\n",
    "print(\"uv min\", torch.min(uv), torch.max(uv))\n",
    "print(\"uv proj min\", torch.min(uv_projection), torch.max(uv_projection))\n",
    "\n",
    "print(\"diff uv\", diff_uv)\n",
    "\n",
    "print(\"uv\", uv)\n",
    "print(\"uv_projection\", uv_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604852b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "latent = torch.randn((1,3, 96,96,96,128))\n",
    "print(latent.shape)\n",
    "\n",
    "latent = latent.mean(1)\n",
    "\n",
    "print(latent.shape)\n",
    "\n",
    "latent = latent.permute(0,4,1,2,3)\n",
    "\n",
    "print(latent.shape)\n",
    "\n",
    "feature = torch.randn((1, 8, 96, 96, 96))\n",
    "\n",
    "print(feature.shape)\n",
    "\n",
    "samples = torch.randn((2000,65,3))\n",
    "\n",
    "samples = samples.view(-1,3)\n",
    "\n",
    "samples = samples.view(1, 1, 1, -1, 3) * 2 - 1.0  # [1 1 H W 3] (x,y,z)\n",
    "print(samples.shape)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "data_im = F.grid_sample(feature, samples, align_corners=True, mode='bilinear')\n",
    "\n",
    "print(\"data im\", data_im.shape)\n",
    "\n",
    "out = data_im.squeeze().permute(1,0)\n",
    "\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cd0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "seg_mask = Image.open('/home/zubairirshad/SAPIEN/renders_balanced/laptop/11586/train/80_degree/seg/r_0.png')\n",
    "seg_mask =  np.array(seg_mask)\n",
    "\n",
    "plt.imshow(seg_mask)\n",
    "plt.show()\n",
    "\n",
    "print(np.unique(seg_mask))\n",
    "# seg_mask[seg_mask>1] =1\n",
    "# plt.imshow(seg_mask)\n",
    "# plt.show()\n",
    "instance_mask = seg_mask >0\n",
    "\n",
    "plt.imshow(instance_mask)\n",
    "plt.show()\n",
    "print(instance_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "embedding = nn.Embedding(91, 32)  # 91 because 0 to 90 inclusive\n",
    "\n",
    "input_tensor = torch.tensor([0])\n",
    "\n",
    "# Pass the input tensor through the embedding module to obtain the embedded representation\n",
    "embedded = embedding(input_tensor)\n",
    "\n",
    "# Check the shape of the embedded tensor\n",
    "print(embedded.shape)  # Output: torch.Size([4, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a438d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f71538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_pl",
   "language": "python",
   "name": "nerf_pl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
